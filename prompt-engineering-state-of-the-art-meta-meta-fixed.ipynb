{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25fc469",
   "metadata": {
    "papermill": {
     "duration": 0.028821,
     "end_time": "2023-07-27T03:52:02.989513",
     "exception": false,
     "start_time": "2023-07-27T03:52:02.960692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Engineered Prompt: Autowrite An Essay About State-of-the-Art Machine Learning Meta Using ArXiv Meta Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4251a8ca",
   "metadata": {
    "papermill": {
     "duration": 0.026219,
     "end_time": "2023-07-27T03:52:03.042405",
     "exception": false,
     "start_time": "2023-07-27T03:52:03.016186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this prompted essay a meta API query will serve as a filter for query selection. This is a meta analysis of meta analyses about machine learning so let's write a few different queries related to machine learning meta studies. We can add more queried data later and build upon the topic-at-hand to broaden the scope of the essay or fine-tune a model to suit our needs. The plan is to make sure the final output essay text contains information about technologies that have been recently published. We then need to know how good the essay is by comparison. To what? If it leaves out a crucial component that would make the difference between a well researched piece and something obviously written by a machine or a neophyte it won't be very useful to research. \n",
    "\n",
    "SPECTER is a pre-trained model for classification and recommendation of scientific papers built on transformers.[1] This is what we want to be told to use if we prompt our chosen essay writing model about the task of building *it* or about recent machine learning research developments and important in-the-know scientific results based on complex meta data summarization. However, we need the model to write about the previous two years of journal articles for our meta analysis and SPECTER is out-of-date since its publication. \n",
    "\n",
    "Hugging Face has alienai/specter2 which is the most recent version of SPECTER. We can play with the [\"bleeding edge\"](http://huggingface.co/docs/transformers/installation) of Hugging Face as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd3b35b",
   "metadata": {
    "papermill": {
     "duration": 0.037864,
     "end_time": "2023-07-27T03:52:03.106817",
     "exception": false,
     "start_time": "2023-07-27T03:52:03.068953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Using cached plotly-6.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: gensim in ./venv/lib/python3.11/site-packages (4.4.0)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Using cached narwhals-2.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from plotly) (25.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in ./venv/lib/python3.11/site-packages (from gensim) (2.4.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in ./venv/lib/python3.11/site-packages (from gensim) (1.16.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in ./venv/lib/python3.11/site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in ./venv/lib/python3.11/site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "Using cached plotly-6.5.0-py3-none-any.whl (9.9 MB)\n",
      "Using cached narwhals-2.14.0-py3-none-any.whl (430 kB)\n",
      "Installing collected packages: narwhals, plotly\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/2\u001b[0m [plotly]\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1/2\u001b[0m [plotly]\n",
      "\u001b[1A\u001b[2KSuccessfully installed narwhals-2.14.0 plotly-6.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install plotly gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5869f30f",
   "metadata": {
    "papermill": {
     "duration": 0.034874,
     "end_time": "2023-07-27T03:52:03.169671",
     "exception": false,
     "start_time": "2023-07-27T03:52:03.134797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b015101",
   "metadata": {
    "papermill": {
     "duration": 49.95491,
     "end_time": "2023-07-27T03:52:53.151527",
     "exception": false,
     "start_time": "2023-07-27T03:52:03.196617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dill==0.3.1.1\n",
      "  Using cached dill-0.3.1.1.tar.gz (151 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cmudict==1.0.2\n",
      "  Using cached cmudict-1.0.2-py2.py3-none-any.whl.metadata (3.7 kB)\n",
      "\u001b[31mERROR: Ignored the following yanked versions: 1.11.0, 1.14.0rc1\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement scipy==1.8.0 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0, 1.6.1, 1.9.0rc1, 1.9.0rc2, 1.9.0rc3, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0rc1, 1.10.0rc2, 1.10.0, 1.10.1, 1.11.0rc1, 1.11.0rc2, 1.11.1, 1.11.2, 1.11.3, 1.11.4, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.13.0rc1, 1.13.0, 1.13.1, 1.14.0rc2, 1.14.0, 1.14.1, 1.15.0rc1, 1.15.0rc2, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.16.0rc1, 1.16.0rc2, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.17.0rc1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for scipy==1.8.0\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"dill==0.3.1.1\" \"cmudict==1.0.2\" \"scipy==1.8.0\" \"importlib-metadata==6.0\" \"numpy==1.23.4\" \"shapely==2.0\" 'pymc3==3.11.0' \"sentence-transformers\" \"accelerate>=0.20.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c37d8920",
   "metadata": {
    "papermill": {
     "duration": 0.052924,
     "end_time": "2023-07-27T03:52:53.245060",
     "exception": false,
     "start_time": "2023-07-27T03:52:53.192136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5150e5c2",
   "metadata": {
    "papermill": {
     "duration": 14.20164,
     "end_time": "2023-07-27T03:53:07.487899",
     "exception": false,
     "start_time": "2023-07-27T03:52:53.286259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from huggingface_hub)\n",
      "  Using cached filelock-3.20.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub)\n",
      "  Using cached fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface_hub)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.11/site-packages (from huggingface_hub) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.11/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from huggingface_hub) (6.0.3)\n",
      "Collecting shellingham (from huggingface_hub)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typer-slim (from huggingface_hub)\n",
      "  Using cached typer_slim-0.21.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.11/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.12.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.9)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.16.0)\n",
      "Requirement already satisfied: click>=8.0.0 in ./venv/lib/python3.11/site-packages (from typer-slim->huggingface_hub) (8.3.1)\n",
      "Using cached huggingface_hub-1.2.3-py3-none-any.whl (520 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl (2.9 MB)\n",
      "Using cached fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.20.1-py3-none-any.whl (16 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached typer_slim-0.21.0-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: typer-slim, tqdm, shellingham, hf-xet, fsspec, filelock, huggingface_hub\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7/7\u001b[0m [huggingface_hub]\u001b[32m6/7\u001b[0m [huggingface_hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.20.1 fsspec-2025.12.0 hf-xet-1.2.0 huggingface_hub-1.2.3 shellingham-1.5.4 tqdm-4.67.1 typer-slim-0.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4286475b",
   "metadata": {
    "papermill": {
     "duration": 21.716704,
     "end_time": "2023-07-27T03:53:29.246560",
     "exception": false,
     "start_time": "2023-07-27T03:53:07.529856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting adapter-transformers\n",
      "  Using cached adapter_transformers-4.0.0-py3-none-any.whl\n",
      "Collecting adapters (from adapter-transformers)\n",
      "  Using cached adapters-1.2.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting transformers~=4.51.3 (from adapters->adapter-transformers)\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from adapters->adapter-transformers) (25.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from transformers~=4.51.3->adapters->adapter-transformers) (3.20.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers~=4.51.3->adapters->adapter-transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.11/site-packages (from transformers~=4.51.3->adapters->adapter-transformers) (2.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from transformers~=4.51.3->adapters->adapter-transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers~=4.51.3->adapters->adapter-transformers)\n",
      "  Using cached regex-2025.11.3-cp311-cp311-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from transformers~=4.51.3->adapters->adapter-transformers) (2.32.5)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers~=4.51.3->adapters->adapter-transformers)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers~=4.51.3->adapters->adapter-transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-macosx_10_12_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.11/site-packages (from transformers~=4.51.3->adapters->adapter-transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers~=4.51.3->adapters->adapter-transformers) (2025.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers~=4.51.3->adapters->adapter-transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers~=4.51.3->adapters->adapter-transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests->transformers~=4.51.3->adapters->adapter-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests->transformers~=4.51.3->adapters->adapter-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests->transformers~=4.51.3->adapters->adapter-transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests->transformers~=4.51.3->adapters->adapter-transformers) (2025.11.12)\n",
      "Using cached adapters-1.2.0-py3-none-any.whl (302 kB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-macosx_10_12_x86_64.whl (2.9 MB)\n",
      "Using cached regex-2025.11.3-cp311-cp311-macosx_10_9_x86_64.whl (290 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-macosx_10_12_x86_64.whl (467 kB)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers, adapters, adapter-transformers\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub9;38;114m\u2578\u001b[0m\u001b[38;5;237m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1/7\u001b[0m [regex]\n",
      "\u001b[2K    Found existing installation: huggingface_hub 1.2.35;237m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1/7\u001b[0m [regex]\n",
      "\u001b[2K    Uninstalling huggingface_hub-1.2.3:;114m\u2578\u001b[0m\u001b[38;5;237m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1/7\u001b[0m [regex]\n",
      "\u001b[2K      Successfully uninstalled huggingface_hub-1.2.38;5;237m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1/7\u001b[0m [regex]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7/7\u001b[0m [adapter-transformers]m \u001b[32m5/7\u001b[0m [adapters]ers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed adapter-transformers-4.0.0 adapters-1.2.0 huggingface-hub-0.36.0 regex-2025.11.3 safetensors-0.7.0 tokenizers-0.21.4 transformers-4.51.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U adapter-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4312bf34",
   "metadata": {
    "papermill": {
     "duration": 13.950643,
     "end_time": "2023-07-27T03:53:43.241567",
     "exception": false,
     "start_time": "2023-07-27T03:53:29.290924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./venv/lib/python3.11/site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from transformers) (3.20.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./venv/lib/python3.11/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.11/site-packages (from transformers) (2.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.11/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.11/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.11/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests->transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests->transformers) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "818fe574",
   "metadata": {
    "papermill": {
     "duration": 34.078555,
     "end_time": "2023-07-27T03:54:17.365334",
     "exception": false,
     "start_time": "2023-07-27T03:53:43.286779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting d6tflow\n",
      "  Using cached d6tflow-0.2.8-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting luigi>=3.0.1 (from d6tflow)\n",
      "  Using cached luigi-3.6.0-py3-none-any.whl\n",
      "Collecting pandas (from d6tflow)\n",
      "  Using cached pandas-2.3.3-cp311-cp311-macosx_10_9_x86_64.whl.metadata (91 kB)\n",
      "Collecting pyarrow (from d6tflow)\n",
      "  Using cached pyarrow-22.0.0-cp311-cp311-macosx_12_0_x86_64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: Jinja2 in ./venv/lib/python3.11/site-packages (from d6tflow) (3.1.6)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.5 in ./venv/lib/python3.11/site-packages (from luigi>=3.0.1->d6tflow) (2.9.0.post0)\n",
      "Collecting tenacity<9,>=8 (from luigi>=3.0.1->d6tflow)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: tornado<7,>=5.0 in ./venv/lib/python3.11/site-packages (from luigi>=3.0.1->d6tflow) (6.5.4)\n",
      "Collecting python-daemon (from luigi>=3.0.1->d6tflow)\n",
      "  Using cached python_daemon-3.1.2-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil<3,>=2.7.5->luigi>=3.0.1->d6tflow) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from Jinja2->d6tflow) (3.0.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./venv/lib/python3.11/site-packages (from pandas->d6tflow) (2.4.0)\n",
      "Collecting pytz>=2020.1 (from pandas->d6tflow)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.11/site-packages (from pandas->d6tflow) (2025.3)\n",
      "Collecting lockfile>=0.10 (from python-daemon->luigi>=3.0.1->d6tflow)\n",
      "  Using cached lockfile-0.12.2-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Using cached d6tflow-0.2.8-py3-none-any.whl (23 kB)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached pandas-2.3.3-cp311-cp311-macosx_10_9_x86_64.whl (11.6 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached pyarrow-22.0.0-cp311-cp311-macosx_12_0_x86_64.whl (36.0 MB)\n",
      "Using cached python_daemon-3.1.2-py3-none-any.whl (30 kB)\n",
      "Using cached lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
      "Installing collected packages: pytz, lockfile, tenacity, python-daemon, pyarrow, pandas, luigi, d6tflow\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m8/8\u001b[0m [d6tflow]\u2501\u2501\u2501\u001b[0m \u001b[32m6/8\u001b[0m [luigi]]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed d6tflow-0.2.8 lockfile-0.12.2 luigi-3.6.0 pandas-2.3.3 pyarrow-22.0.0 python-daemon-3.1.2 pytz-2025.2 tenacity-8.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install d6tflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abdc1af4",
   "metadata": {
    "papermill": {
     "duration": 16.276233,
     "end_time": "2023-07-27T03:54:33.688350",
     "exception": false,
     "start_time": "2023-07-27T03:54:17.412117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading postgres module without psycopg2 nor pg8000 installed. Will crash at runtime if postgres functionality is used.\n",
      "Loading S3 module without the python package boto3. Will crash at runtime if S3 functionality is used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to d6tflow! For Q&A see https://github.com/d6t/d6tflow\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparsing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m remove_stopwords\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstring\u001b[39;00m \n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import d6tflow \n",
    "import requests\n",
    "import os\n",
    "import pandas as pd \n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import string \n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pickle\n",
    "import json\n",
    "from collections import Counter, OrderedDict\n",
    "import operator\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a5a03",
   "metadata": {
    "papermill": {
     "duration": 0.056948,
     "end_time": "2023-07-27T03:54:33.792828",
     "exception": false,
     "start_time": "2023-07-27T03:54:33.735880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "# Subtract two years\n",
    "two_years_ago = now - relativedelta(years=2)\n",
    "print(two_years_ago)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccabb2fe",
   "metadata": {
    "papermill": {
     "duration": 0.046999,
     "end_time": "2023-07-27T03:54:33.886403",
     "exception": false,
     "start_time": "2023-07-27T03:54:33.839404",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We'll come back to the Hugging Face transformers library after we construct some dataframes to analyze. ArXiv has a nice API to harness articles by query. These results may be slightly more up-to-date than the official competion article meta dataset. This is \"the bleeding edge\" anyhow. This notebook is not competing in the current Kaggle essay contest. \n",
    "\n",
    "What have we done so far? \n",
    "\n",
    "The two things to focus attention on are the libraries for Hugging Face and d6tflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5ab73",
   "metadata": {
    "papermill": {
     "duration": 0.05838,
     "end_time": "2023-07-27T03:54:33.991302",
     "exception": false,
     "start_time": "2023-07-27T03:54:33.932922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_words = ['machine', 'learning', 'meta', 'data']\n",
    "queries = [word + '&' for word in query_words]\n",
    "query = ''.join(queries)\n",
    "query_size = 3000\n",
    "url = f'https://export.arxiv.org/api/query?search_query=all:{query}start=0&max_results={query_size}'\n",
    "print(url)\n",
    "word = query_words[:2]\n",
    "word = ' '.join(word)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bb1a15",
   "metadata": {
    "papermill": {
     "duration": 0.046606,
     "end_time": "2023-07-27T03:54:34.084702",
     "exception": false,
     "start_time": "2023-07-27T03:54:34.038096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, we can get the meta data from the articles about our query for our query of terms from our url. Let's find articles from within the past two years. For our task management there is a very useful package to leverage machine learning pipelining and data science workflows called d6tflow.\n",
    "The search query is returning 3000 results which is a managable amount for our meta use case. If you decide to run this notebook, keep in mind that obsessively researching your favorite queries is a beneficial but maybe intensive hobby. This prompt engine is very useful for understanding publication vectors for string choices that signify interest areas not too broad and not too narrow. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e5b0b",
   "metadata": {
    "papermill": {
     "duration": 16.212036,
     "end_time": "2023-07-27T03:54:50.345186",
     "exception": false,
     "start_time": "2023-07-27T03:54:34.133150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = requests.get(url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44470c63",
   "metadata": {
    "papermill": {
     "duration": 0.046545,
     "end_time": "2023-07-27T03:54:50.439500",
     "exception": false,
     "start_time": "2023-07-27T03:54:50.392955",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Saving a dict object to a pickle with a d6tflow task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049d628",
   "metadata": {
    "papermill": {
     "duration": 0.087804,
     "end_time": "2023-07-27T03:54:50.574025",
     "exception": false,
     "start_time": "2023-07-27T03:54:50.486221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MetaDataTask(d6tflow.tasks.TaskPickle):\n",
    "    def run(self):\n",
    "        archive = dict()\n",
    "        archive['url'] = url \n",
    "        archive[\"data\"] = data\n",
    "        df = {'meta': archive}\n",
    "        self.save(df)\n",
    "        \n",
    "flow = d6tflow.Workflow()\n",
    "flow.run(MetaDataTask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d491c9f1",
   "metadata": {
    "papermill": {
     "duration": 1.504716,
     "end_time": "2023-07-27T03:54:52.129052",
     "exception": true,
     "start_time": "2023-07-27T03:54:50.624336",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "class SortMetaTask(d6tflow.tasks.TaskPickle):\n",
    "    def run(self):\n",
    "        two_years_ago = pd.Timestamp.now(tz='UTC') - pd.DateOffset(years=2)\n",
    "        df = pd.read_xml(StringIO(data))\n",
    "        xml_df = pd.DataFrame()\n",
    "        xml_df['title'] = df['title'][7:]\n",
    "        xml_df['abstract'] = df['summary'][7:]\n",
    "        xml_df['published'] = df['published'][7:]\n",
    "        xml_df['published'] = pd.to_datetime(df['published'])\n",
    "        xml_df['updated'] = df['updated'][7:]\n",
    "        xml_df['url'] = df['id'][7:]\n",
    "        xml_df['two_year_date'] = xml_df['published'].apply(lambda x: 1 if x > two_years_ago else 0)\n",
    "        xml_df['title_has_word'] = xml_df['title'].str.contains(f'{word}', case=False)\n",
    "        xml_df['combined'] = xml_df['title'] + ' ' + xml_df['abstract']\n",
    "        index = pd.RangeIndex(start=0, stop=len(xml_df))\n",
    "        xml_df.index = index\n",
    "        self.save(xml_df)\n",
    "        \n",
    "flow = d6tflow.Workflow()\n",
    "flow.run(SortMetaTask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcae743",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We now have a beautiful addition to our working directory as d6tflow automatically creates a data directory with subdiretories for each task you run that returns a saved object successfully. Our new meta data archive based on the queries we made is stored in a pickle file. \n",
    "\n",
    "If we save data in the data directory and file format using a d6tflow Task object and then want to delete and rerun to repopulate our data directory we will have to remove it first. Once files are saved by d6tflow they will be immutable. So, they must be deleted before any errors can be corrected. The working_directory() call will list our d6tflow created directories and files in the Kaggle working tree. The data_reset() call will wipe the data directory clean allowing a fresh rerun of all the tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac27385",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "def working_directory():\n",
    "    for dirname, _, filenames in os.walk('/kaggle/working'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "\n",
    "# Clear data folder\n",
    "\n",
    "def data_reset(directory):\n",
    "    for the_file in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                data_reset(file_path)\n",
    "                os.rmdir(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "data_path = '/kaggle/working/' + f\"{data_dir}\"\n",
    "# data_reset(data_path)\n",
    "working_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae897f0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip freeze>requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c14c7ca",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We have added a requirements.txt based on some dependency issues and we can now rerun the above cell when an update to the .txt file is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8881661e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('paraphrase-albert-small-v2')\n",
    "flow = d6tflow.Workflow()\n",
    "df = flow.outputLoad(SortMetaTask)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc16435",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# **A little sentence transformer, a pickle, and a dataframe walk into a bar...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6918bc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClusterTask(d6tflow.tasks.TaskPickle):\n",
    "    def run(self):\n",
    "        # Generate embeddings for all titles\n",
    "        embeddings = model.encode(df['title'])\n",
    "        # Cluster embeddings into 5 clusters (adjust this value based on your data)\n",
    "        kmeans = KMeans(n_clusters=5)\n",
    "        df['cluster'] = kmeans.fit_predict(embeddings)\n",
    "        grouped = df.groupby(['published', 'cluster']).size().reset_index(name='counts')\n",
    "        # Create a line plot for each topic\n",
    "        fig = px.scatter(grouped, x='published', y='cluster', color='cluster')\n",
    "        fig.update_layout(\n",
    "            title_text=f'Title Clusters For Query: {query_words}', \n",
    "            xaxis_title_text='Date', \n",
    "            yaxis_title_text=f'5 Clusters For Results', \n",
    "        )\n",
    "        fig.show()\n",
    "        self.save(df)\n",
    "        \n",
    "flow = d6tflow.Workflow()\n",
    "flow.run(ClusterTask)\n",
    "\n",
    "\n",
    "def count_vect():\n",
    "    # Use a CountVectorizer to count word frequencies\n",
    "    documents = df['title']\n",
    "    labels = df['cluster']\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "\n",
    "    for i in range(5):  # for each cluster\n",
    "        cluster_docs = np.array(documents)[np.array(labels) == i]\n",
    "        cluster_X = vectorizer.fit_transform(cluster_docs)\n",
    "        word_freq = np.sum(cluster_X, axis=0)\n",
    "        word_freq = np.asarray(word_freq).ravel().tolist()\n",
    "        # features = cluster_X.get_feature_names()\n",
    "        word_freq_df = pd.DataFrame({'frequency': word_freq})\n",
    "        # print(f\"Most common terms in cluster {i}:\")\n",
    "        print(word_freq_df.sort_values(by='frequency', ascending=False).head())\n",
    "        # Get the word for each counted index\n",
    "        # words = {word: counts[idx] for word, idx in X.vocabulary.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf607fdd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We can render some basic visuals from our raw data with some preprocessing. However, this is optional and the real power of this generated essay we are prompting is in SPECTER. \n",
    "\n",
    "Our query is visualized below with a bar graph. The articles results of our original ArXiv query words have been cleaned for word frequencies which includes punctuation removal and sorting each word by frequency. This is reusable code and any words may be substituted for the query we are currently working with for this query based prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eabaf7",
   "metadata": {
    "_kg_hide-input": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def punc_remove(text):\n",
    "    # Remove punctuation \n",
    "    # text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text \n",
    "\n",
    "def freqs(text):\n",
    "    # Convert string to list of words, count, sort in reverse \n",
    "    text = text.split()\n",
    "    freq_list = [(w, text.count(w)) for w in text] \n",
    "    freq_list.sort(key=lambda w: w[1], reverse=True)\n",
    "    # Duplicates removal from the list \n",
    "    l = []\n",
    "    [l.append(x) for x in freq_list if x not in l]\n",
    "    # Converting list of tuples into two lists\n",
    "    words, frequencies = zip(*l)\n",
    "    # Create a bar plot\n",
    "    title_text = f'ArXiv Two Prior Years Top Ten KeyWord Frequencies For: {query_words}'\n",
    "    # query_size = f'{query_size}'\n",
    "    fig = px.bar(x=words[:10], y=frequencies[:10], \n",
    "                 labels={'x':'Word', 'y':f'Combined Occurences Titles and Abstract For Past Two Years'}, \n",
    "                 title=title_text)\n",
    "    fig.show()\n",
    "    return l \n",
    "\n",
    "# This will load our cached XML dataframe with five sorted columns; 0-4\n",
    "\n",
    "class VizMetaTask(d6tflow.tasks.TaskCachePandas):\n",
    "    def requires(self):\n",
    "        return ClusterTask()\n",
    "    \n",
    "    def run(self):\n",
    "        data = self.inputLoad()\n",
    "        df = pd.DataFrame(data)\n",
    "        df = df[df['two_year_date'] == 1]\n",
    "        # Remove puntuation and stop words \n",
    "        df['combined'] = df['combined'].str.lower()\n",
    "        title_text = df['title']\n",
    "        abstract_text = df['abstract']\n",
    "        # Calls take a single string \n",
    "        text = df['combined'].str.cat(sep=' ')\n",
    "        text = punc_remove(text)\n",
    "        text = remove_stopwords(text)\n",
    "        # Top word frequencies in abstracts \n",
    "        abstract_freq = freqs(text)\n",
    "        abstract_df = pd.DataFrame(abstract_freq)\n",
    "        self.save(df)\n",
    "        \n",
    "# flow = d6tflow.Workflow()\n",
    "# flow.run(VizMetaTask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b36f02c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Sweet deal. \n",
    "We now have a beautiful graph of the top ten word frequencies for the prior two years ArXiv articles for the given query. \n",
    "Since this essay is ultimately an analysis of descriptive text summaries written in paragraphs let's explore the Sentence-Transformers library. There is a conversion of SPECTER to a [sentence-transformers model](http://huggingface.co/sentence-transformers/allenai-specter) which maps scientific articles to a vector space for similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c1f45",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "word = query_words[:2]\n",
    "word = ' '.join(word)\n",
    "df = df[df['two_year_date'] == 1]\n",
    "# Binary flag indicating whether \"machine learning\" appears in the title\n",
    "df_agg = df.groupby('published').agg({'title_has_word': 'sum'}).reset_index()\n",
    "unique_dates = df[df['title_has_word'] == 1]['published'].dt.date.nunique()\n",
    "# The count of titles with \"machine learning\" over time\n",
    "count = df['title_has_word'].sum()\n",
    "fig = px.line(df_agg, x='published', y='title_has_word')\n",
    "fig.update_layout(\n",
    "    title_text=f\"{unique_dates} Total Dates of '{word}' Occurances in Titles Over Two Years\", \n",
    "    xaxis_title_text='Date', \n",
    "    yaxis_title_text=f\"ArXiv Binary Flag for '{word}' in Title\", \n",
    "    bargap=0.2,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Aggregate the DataFrame by date, summing up the 'title_has_word' column for each date\n",
    "df_agg = df.groupby(df['published'].dt.date).agg({'title_has_word': 'sum'}).reset_index()\n",
    "ml_sum = df_agg['title_has_word'].sum()\n",
    "ml_sum = ml_sum.copy()\n",
    "# Plot the count of titles containing 'machine learning' over time\n",
    "fig = px.line(df_agg, x='published', y='title_has_word')\n",
    "fig.update_layout(\n",
    "    title_text=f\"{ml_sum} '{word}' Titles Over The Past Two Years\", \n",
    "    xaxis_title_text='Date', \n",
    "    yaxis_title_text=f\"{ml_sum} ArXiv Title Publication Count for '{word}'\", \n",
    "    bargap=0.2,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf868c19",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO huggingface_hub cedentials?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b2fccf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model)\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "models = api.list_models()\n",
    "df_models = pd.DataFrame(models)\n",
    "# The 6th in the list of models \n",
    "print(df_models[0][5])\n",
    "model = SentenceTransformer('')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d13c57",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://huggingface.co/api/models'\n",
    "data = requests.get(url).text\n",
    "# xml = pd.read_xml()\n",
    "# print(xml)\n",
    "d = data.rindex('specter', 243203)\n",
    "print(d, type(d), data[d-245:d+24])\n",
    "# This appears to be the only specter model available now. SMH\n",
    "# AnonymousSub/specter-emanuals-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c684b5f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flow = d6tflow.Workflow()\n",
    "df = flow.outputLoad(SortMetaTask)\n",
    "def get_item(row):\n",
    "    return row[2]\n",
    "def expand_df(df):\n",
    "    # Needs to be sorted again \n",
    "    df['sort_key'] = df['articles'].apply(get_item)\n",
    "    df.sort_values('sort_key', inplace=True)\n",
    "    df.drop(columns='sort_key', inplace=True)\n",
    "    df_expanded = df['articles'].apply(pd.Series)\n",
    "    df_expanded.columns = ['url', 'update', 'pub date', 'title', 'abstract']\n",
    "    return df_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f1e327",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def xml_data(xml):\n",
    "    root = ET.fromstring(xml)\n",
    "    archive = {'articles': []}\n",
    "    for children in root:\n",
    "        for child in children[2:3]:\n",
    "            archive['articles'].append([child.text for child in children[:5]])\n",
    "    data = pd.DataFrame(archive)\n",
    "    df = expand_df(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be6d2e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "working_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7a9cc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# The following embedding API - https://github.com/allenai/paper-embedding-public-apis - \n",
    "# will embed batches up to size 16 with SPECTER. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e4682",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "URL = \"https://model-apis.semanticscholar.org/specter/v1/invoke\"\n",
    "MAX_BATCH_SIZE = 16\n",
    "\n",
    "def chunks(lst, chunk_size=MAX_BATCH_SIZE):\n",
    "    \"\"\"Splits a longer list to respect batch size\"\"\"\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i : i + chunk_size]\n",
    "\n",
    "PAPERS = [\n",
    "    {\n",
    "        \"paper_id\": str(row_id),\n",
    "        \"title\": row['title'],\n",
    "        \"abstract\": row['abstract'],\n",
    "    } \n",
    "    for row_id, row in df.iterrows()\n",
    "]\n",
    "\n",
    "def embed(papers):\n",
    "    embeddings_by_paper_id: Dict[str, List[float]] = {}\n",
    "    for chunk in chunks(papers):\n",
    "        # Allow Python requests to convert the data above to JSON\n",
    "        response = requests.post(URL, json=chunk)\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(\"Sorry, something went wrong, please try later!\")\n",
    "        for paper in response.json()[\"preds\"]:\n",
    "            embeddings_by_paper_id[paper[\"paper_id\"]] = paper[\"embedding\"]\n",
    "    return embeddings_by_paper_id\n",
    "\n",
    "class Embeddings2Task(d6tflow.tasks.TaskPickle):\n",
    "    def run(self):\n",
    "        all_embeddings = embed(PAPERS)\n",
    "        self.save(all_embeddings)\n",
    "    \n",
    "flow = d6tflow.Workflow()\n",
    "flow.run(Embeddings2Task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5d5b7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "There's now a very nice new .pkl and an updated .json dataset in our working tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb490707",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Alright, we have wrangled some very nice embeddings that can be queried for similar scientific papers with a two part query that expects a title and an abstract to search for throughout the pretrained model corpora. We can get by for now with some short descriptive text in place of a title and abstract. \n",
    "\n",
    "We will update our dataset for this model with our original MetaDataTask output so that we can have search results for the gap years prior to two years ago and 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536be64",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SecondSortTask(d6tflow.tasks.TaskPickle):\n",
    "    def requires(self):\n",
    "        return MetaDataTask()\n",
    "    \n",
    "    def run(self): \n",
    "        df = self.inputLoad()\n",
    "        df = df['meta']['data']\n",
    "        root = ET.fromstring(df)\n",
    "        archive = {'articles': []}\n",
    "        date_list = []\n",
    "        abstract_list = []\n",
    "        word = query_words[:2]\n",
    "        word = ' '.join(word)\n",
    "        \n",
    "        for children in root:\n",
    "            for child in children[2:3]:\n",
    "                date_list.append(child.text)\n",
    "\n",
    "            for child in children[3:4]:\n",
    "                archive['articles'].append(child.text)\n",
    "\n",
    "            for child in children[4:5]:\n",
    "                abstract_list.append(child.text)\n",
    "                \n",
    "        archive['date'] = date_list\n",
    "        df = pd.DataFrame()\n",
    "        df['date'] = archive['date']\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df['title'] = archive['articles']\n",
    "        df['abstract'] = abstract_list\n",
    "        df['abstract'] = df['abstract'].str.lower()\n",
    "        df['abstract'] = df['abstract'].replace('\\\\n','', regex=True)\n",
    "        df['title'] = df['title'].str.lower()\n",
    "        df['contains_search_string'] = df['title'].str.contains(word)\n",
    "        df = df.sort_values(by='date')\n",
    "        df['title'] = df['title'].replace('\\\\n','', regex=True)\n",
    "        df['title'] = df['title'].fillna('')\n",
    "        df = df[df['title'].str.strip() != '']\n",
    "        \n",
    "        # Initialize SVD columns with NaN\n",
    "        df['svd_x'] = np.nan\n",
    "        df['svd_y'] = np.nan\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            print('Warning: No valid data after filtering')\n",
    "            self.save(df)\n",
    "            return\n",
    "\n",
    "        fig = px.histogram(df, x='date')\n",
    "        fig.update_layout(\n",
    "            title_text=f'Distribution of Article Dates For Query: {query_words}', \n",
    "            xaxis_title_text='6 Month Period', \n",
    "            yaxis_title_text=f'Count For {query_size} Results', \n",
    "            bargap=0.3,\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "        search_counts = df['contains_search_string'].value_counts()\n",
    "        true_count = search_counts.get(True, 0)\n",
    "        false_count = search_counts.get(False, 0)\n",
    "        counts = df.groupby('date')['contains_search_string'].sum().reset_index()\n",
    "        \n",
    "        if true_count > 0:\n",
    "            fig = px.histogram(counts, x='date', y ='contains_search_string', nbins=50)\n",
    "            fig.update_layout(\n",
    "                title_text=f'{true_count} \\\"{word}\\\" Article Titles Distribution by Date', \n",
    "                xaxis_title_text='Year', \n",
    "                yaxis_title_text=f'ArXiv Article Title Count For \\\"{word}\\\"', \n",
    "                bargap=0.2,\n",
    "            )\n",
    "            fig.show()\n",
    "        \n",
    "        trace1 = go.Histogram(x=df['date'], opacity=1.0, name='Your ArXiv Search Query Results By Year')\n",
    "        trace2 = go.Histogram(x=counts.loc[counts['contains_search_string'] == True, 'date'], nbinsx=50, opacity=0.75, name=f'{word} is in Result Title')\n",
    "        fig = go.Figure(data=[trace1, trace2], layout=go.Layout(barmode='group'))\n",
    "        fig.show()\n",
    "\n",
    "        if len(df) > 0:\n",
    "            vectorizer = CountVectorizer()\n",
    "            X = vectorizer.fit_transform(df['title'])\n",
    "            svd = TruncatedSVD(n_components=2)\n",
    "            X_2d = svd.fit_transform(X)\n",
    "            df['svd_x'] = X_2d[:, 0]\n",
    "            df['svd_y'] = X_2d[:, 1]\n",
    "\n",
    "            fig = go.Figure()\n",
    "            if false_count > 0:\n",
    "                fig.add_trace(go.Scatter(x=df[~df['contains_search_string']]['svd_x'], y=df[~df['contains_search_string']]['svd_y'], mode='markers', name=f'{false_count} Titles w/o \\\"{word}\\\"', hovertext=df[~df['contains_search_string']]['title']))\n",
    "\n",
    "            if true_count > 0:\n",
    "                fig.add_trace(go.Scatter(x=df[df['contains_search_string']]['svd_x'], y=df[df['contains_search_string']]['svd_y'], mode='markers', name=f'{true_count} Titles Containing \\\"{word}\\\"', hovertext=df[df['contains_search_string']]['title']))\n",
    "            \n",
    "            fig.update_layout(title_text=f'Hover Title Text SVD Scatter Components', xaxis_title=\"Component 1\", yaxis_title=\"Component 2\", font=dict(size=18))\n",
    "            fig.show()\n",
    "        \n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            for i, component in enumerate(svd.components_):\n",
    "                top_words_idx = component.argsort()[-10:][::-1]\n",
    "                top_words = [feature_names[idx] for idx in top_words_idx]\n",
    "                print(f\"Component {i+1} top words: {top_words}\")\n",
    "            \n",
    "        self.save(df)\n",
    "        \n",
    "# flow = d6tflow.Workflow()\n",
    "# flow.run(SecondSortTask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc97f6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flow = d6tflow.Workflow()\n",
    "df = flow.outputLoad(SecondSortTask)\n",
    "all_embeddings = flow.outputLoad(Embeddings2Task)\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681caeb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize SPECTER embeddings \n",
    "embedded_df = pd.DataFrame(all_embeddings)\n",
    "embedded_df.to_json('specter_embeddings.json')\n",
    "# Transpose the dataframe, as PCA expects rows to represent samples (documents in this case)\n",
    "df_transposed = embedded_df.transpose()\n",
    "pca = PCA(n_components=2)\n",
    "# Apply PCA to the embeddings\n",
    "reduced_embeddings = pca.fit_transform(df_transposed)\n",
    "df_reduced = pd.DataFrame(reduced_embeddings, columns=['Dimension 1', 'Dimension 2'])\n",
    "df_reduced[f'Contains Search String {word}'] = df['contains_search_string'].values\n",
    "df_r_len = len(df_reduced)\n",
    "# Make 'date' column and two_years_ago date timezone naive\n",
    "df['date'] = df['date'].dt.tz_localize(None)\n",
    "two_years_ago = two_years_ago.replace(tzinfo=None)\n",
    "print(df_r_len)\n",
    "df_reduced['Published Within Last Two Years'] = df['date'].apply(lambda x: x > two_years_ago)\n",
    "df_reduced['Not Published Within Last Two Years'] = df['date'].apply(lambda x: x < two_years_ago)\n",
    "true_count = df_reduced['Published Within Last Two Years'].value_counts()\n",
    "print(true_count)\n",
    "df_len = len(df_reduced)\n",
    "df_new = df_reduced[df_reduced['Not Published Within Last Two Years'] == False]\n",
    "two_counts = df_reduced['Not Published Within Last Two Years'].value_counts()\n",
    "print(df_new, two_counts)\n",
    "date = df['date']\n",
    "fig = px.scatter(df_reduced, x=date, y='Dimension 2', color=f'Contains Search String {word}')\n",
    "fig.update_layout(\n",
    "        title_text=f'1D (Dimension 2) PCA Scatter For All {df_r_len} SPECTER Embeddings'\n",
    "      \n",
    ")    \n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(df_reduced, x='Dimension 1', y='Dimension 2', color=f'Contains Search String {word}')\n",
    "fig.update_layout(\n",
    "        title_text=f' PCA Scatter For All {df_r_len} SPECTER Embeddings'\n",
    ")    \n",
    "fig.show()   \n",
    "\n",
    "fig = px.scatter(df_reduced, x='Dimension 1', y='Dimension 2', color='Published Within Last Two Years')\n",
    "fig.update_layout(\n",
    "        title_text=f'2D PCA Scatter For {df_r_len} Embeddings'\n",
    ")    \n",
    "fig.show()\n",
    "df_new_len = len(df_new)\n",
    "fig = px.scatter(df_new, x='Dimension 1', y='Dimension 2', color=f'Contains Search String {word}')\n",
    "fig.update_layout(\n",
    "        title_text=f'2D PCA Scatter For Within Two Years, Total Count: {df_new_len}'\n",
    ")    \n",
    "fig.show()\n",
    "\n",
    "df_new = df_reduced[df_reduced['Not Published Within Last Two Years'] == True]\n",
    "df_new_len = len(df_new)\n",
    "fig = px.scatter(df_new, x='Dimension 1', y='Dimension 2', color=f'Contains Search String {word}')\n",
    "fig.update_layout(\n",
    "        title_text=f'2D PCA Scatter For {word} Embeddings Prior To Two Years Ago From, Total Count: {df_new_len}'\n",
    ")    \n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8615347",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = 'search_results.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb1b76",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = 'specter_embeddings.json'\n",
    "df = pd.read_json(embeddings)\n",
    "df = pd.DataFrame(df)\n",
    "corpus_embeddings = df.transpose().values.tolist()\n",
    "corpus_embeddings = torch.tensor(corpus_embeddings)\n",
    "print(corpus_embeddings)\n",
    "\n",
    "\n",
    "def sentence_transformer(text):\n",
    "    model = SentenceTransformer('sentence-transformers/allenai-specter')\n",
    "    embedding = model.encode(text, convert_to_tensor=True)\n",
    "    return embedding \n",
    "\n",
    "\n",
    "class SPECTERTask(d6tflow.tasks.TaskPickle):   \n",
    "    def requires(self):\n",
    "        return {'task': ClusterTask(), \n",
    "                'task1': MetaDataTask(),\n",
    "               } \n",
    "    # Just a different way to load from a dict of previous task outputs    \n",
    "    def run(self):\n",
    "        data = self.input()['task'].load()\n",
    "        data_task1 = self.input()['task1'].load()\n",
    "\n",
    "        xml = data_task1['meta']['data']\n",
    "        df_xml = xml_data(xml)\n",
    "        df_xml.to_json('sorted_results.json')\n",
    "        # Two years of embeddings as a single embedding\n",
    "        df = data\n",
    "        df['combined_has_word'] = df['combined'].str.contains(f'{word}')\n",
    "        print(df)\n",
    "        two_years = df[df['two_year_date'] == 1]\n",
    "        text = two_years['combined'].tolist()\n",
    "        two_year_embedding = sentence_transformer(text)\n",
    "        # Prior to two years ago for each value of contains {word}  \n",
    "        df = df[df['two_year_date'] == 0]        \n",
    "        text = df[df['combined_has_word'] == False]['combined'].values\n",
    "        array0_embedding = sentence_transformer(text)\n",
    "        text = df[df['combined_has_word'] == True]['combined'].values\n",
    "        array1_embedding = sentence_transformer(text)\n",
    "        data = {'embedding': two_year_embedding, 'array0_embedding': array0_embedding, 'array1_embedding': array1_embedding}\n",
    "        self.save(data)\n",
    "        \n",
    "    # The following modified function can be found here: \n",
    "    # https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/semantic-search/semantic_search_publications.py\n",
    "\n",
    "    def semantic_search_papers(title, abstract):\n",
    "        # SPECTERTask.search_papers() is going to take two strings\n",
    "        # ['url']['title']['abstract']['venue']['year'] for .json  \n",
    "        papers = pd.read_json(results)\n",
    "        papers = pd.DataFrame(papers)\n",
    "        text = title+'[SEP]'+abstract\n",
    "        # df = pd.DataFrame(papers)\n",
    "        query_embedding = sentence_transformer(text)\n",
    "        search_hits = util.semantic_search(query_embedding, corpus_embeddings)\n",
    "        search_hits = search_hits[0] \n",
    "        print(search_hits)  \n",
    "        print(\"\\n\\nPaper:\", title)\n",
    "        print(\"Most similar papers:\")\n",
    "        for hit in search_hits:\n",
    "            i = hit['corpus_id']\n",
    "            related_paper = papers.loc[i]\n",
    "            print(\"{:.2f}\\t{}\\t{} {}\".format(hit['score'], related_paper['title'], related_paper['venue'], related_paper['year']))\n",
    "            \n",
    "    def specter2_search_papers(title, abstract):\n",
    "        # results = 'search_results.json'\n",
    "        # papers = pd.read_json(results)\n",
    "        # papers = pd.DataFrame(papers)\n",
    "        papers = pd.read_json(results)\n",
    "        papers = pd.DataFrame(papers)\n",
    "        papers = papers.to_dict()\n",
    "        papers_zip = zip(papers['title'], papers['abstract'])\n",
    "        print(papers_zip)\n",
    "        model = AutoAdapterModel.from_pretrained(\"alienai/specter2\")\n",
    "        adapter_name = model.load_adapter(\"alienai/specter2_proximity\", source=\"hf\", set_active=True)\n",
    "        # load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained('alienai/specter2')\n",
    "        text_batch = [f\"{title}{tokenizer.sep_token}{abstract}\" for title, abstract in papers_zip]\n",
    "\n",
    "        # preprocess the input\n",
    "        inputs = tokenizer(text_batch, padding=True, truncation=True,\n",
    "                                     return_tensors=\"pt\", return_token_type_ids=False, max_length=512)\n",
    "        output = adapter_name(**inputs)\n",
    "        # take the first token in the batch as the embedding\n",
    "        corpus_embeddings = output.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        \n",
    "\n",
    "flow = d6tflow.Workflow()\n",
    "flow.run(SPECTERTask)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3cec3c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d03c8d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "\\begin{equation} \\cos ({\\bf t},{\\bf e})= {{\\bf t} {\\bf e} \\over \\|{\\bf t}\\| \\|{\\bf e}\\|} = \\frac{ \\sum_{i=1}^{n}{{\\bf t}_i{\\bf e}_i} }{ \\sqrt{\\sum_{i=1}^{n}{({\\bf t}_i)^2}} \\sqrt{\\sum_{i=1}^{n}{({\\bf e}_i)^2}} } \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238a34a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Papers Triplet For Citation Based Model Training \n",
    "$\\begin{equation}$\n",
    "$\\mathcal{L}$ = \n",
    "$\\max \\left\\{ \\left( d(P_Q, P_+)  - d(P_Q, P_-) + m \\right), 0 \\right\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f5411f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "With the SPECTER transformers pretrained on document relatedness they can do inference with just the title and abstract text of the paper used as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f996a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MasterTask(d6tflow.tasks.TaskPqPandas): \n",
    "    def requires(self):\n",
    "        return {'task1': MetaDataTask(), \n",
    "                # 'task2': SortMetaTask(), \n",
    "                # 'task3': ClusterTask(),\n",
    "                'task4': SecondSortTask(), \n",
    "                'task5': VizMetaTask(), \n",
    "               }\n",
    "\n",
    "    def run(self):\n",
    "        data_task1 = self.input()['task1'].load()\n",
    "        # data_task2 = self.input()['task2'].load()\n",
    "        # data_task3 = self.input()['task3'].load()\n",
    "        data_task4 = self.input()['task4'].load()\n",
    "        data_task5 = self.input()['task5'].load()\n",
    "        data = {}\n",
    "       \n",
    "\n",
    "flow = d6tflow.Workflow()\n",
    "flow.run(MasterTask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7c3f0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flow = d6tflow.Workflow()\n",
    "df = flow.outputLoad(SPECTERTask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f551730a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# For exploring SPECTER further, try the alienai/specter github page - [https://github.com/allenai/specter](http://) \n",
    "Below is a visualization of the entire embedding space created by combining for the prior two years worth of results all title and abstract string text into one string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb7fd33",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "specter_embeddings = flow.outputLoad(SPECTERTask)\n",
    "print(specter_embeddings)\n",
    "# calculate cosine similarity\n",
    "embedding = specter_embeddings['embedding']\n",
    "array0_embedding = specter_embeddings['array0_embedding']\n",
    "cosine_scores = util.pytorch_cos_sim(embedding, array0_embedding)\n",
    "\n",
    "print('Cosine Scores: ', cosine_scores)\n",
    "\n",
    "\n",
    "def embeddings_array(embeddings):\n",
    "    indices = [[i] for i in range(len(embeddings.values))]\n",
    "    embeddings_tensor = torch.tensor(embeddings.values)\n",
    "    embeddings_tensor_2d = embeddings_tensor.unsqueeze(0)\n",
    "    embeddings_array_2d = embeddings_tensor_2d.numpy()\n",
    "    indices = np.array(indices)\n",
    "    embeddings_matrix = np.array(embeddings_array_2d)\n",
    "    # print(embeddings_matrix)\n",
    "\n",
    "# embeddings_array(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b7aa72",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "If the similarity is greater for the prior two years whole embedding and the prior to two years embedding of article titles and abstracts with \"machine learning\" in the text, then we know that articles from the prior two years have more semantic similarity with \"machine learning\" in the title and abstract texts than to articles without it prior to two years ago which tells us something about the last two years represented in the query results articles. However, the similarity between two sets prior to two years ago is lower than each against the whole recent two year set of articles and the scores for them against the two year set are very close. \n",
    "\n",
    "What we want to be sure of is whether articles from the last two years that don't contain \"machine learning\" in the text are more similar to older articles that don't or more recent articles that do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80173ee3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8a57c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flow = d6tflow.Workflow()\n",
    "df = flow.outputLoad(SecondSortTask)\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b34c0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "two_years_ago.date()\n",
    "# The regular indices are not sorted \n",
    "print(df['date'][0])\n",
    "# The iloc indices are sorted \n",
    "print(df['date'].iloc[0])\n",
    "print(df['date'].iloc[-1])\n",
    "oldest = df['date'].iloc[0]\n",
    "newest = df['date'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d02be",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_string = f'\"{word}\" occured on {unique_dates} different days in {ml_sum} titles within two years-to-date for the {query_size} results \\\n",
    "which spanned '\n",
    "prompt_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b4424",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_string = prompt_string + str(oldest) + ' to ' + str(newest)\n",
    "prompt_string = prompt_string + f' and two years ago began {two_years_ago}.'\n",
    "print(prompt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b23ec3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytz import utc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f538e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "It's immediately apparent that the results from the previous two years are more scattered over the whole shape of all 3000 embeddings in PCA. Prior to that period, the embeddings appear sparsely grouped into containing or not containing 'machine learning' in the title or abstract text. The clusters are noticible and this description is something I would like to see an LLM tackle in some decent nomenclature. It is also the kind of thing that cannot be used in place of a human when a human is taking ethical documented credit of their own thoughts, evaluations, conclustions, etc. in scientific research. This notebook is a helpful code document, but it's not a replacement for yourself. Learning from it is a less severe infraction than letting it do all the work. Learning from it is doable, but not necissarily dependable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec11c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flow = d6tflow.Workflow()\n",
    "beds = flow.outputLoad(SPECTERTask)\n",
    "embeds = beds['embedding']\n",
    "print(embeds[:8], len(embeds), 'dimensions for two years prior articles as one SPECTER embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82f024c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Rather than compute the embeddings x embeddings matrix for pairwise cosine similarity, we can save some memory by trying Sentence Transformers [paraphrase-mining](https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/paraphrase-mining) which utilizes chunking and prioritizing the best similar pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0e9c5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Let's find the articles with the best matches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1addab",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "SPECTER results are expected to be slightly better (whatever that means) than the paraphrase miner, but this should be interesting nonetheless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fd293",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = (df['title'] + '. ' + df['abstract']).tolist()\n",
    "# paraphrases = util.paraphrase_mining(model, sentences, corpus_chunk_size=len(sentences), top_k=1)\n",
    "# paraphrase-albert-small-v2 has tougher scores and is the smallest \n",
    "class ParaphraseTask(d6tflow.tasks.TaskPickle):\n",
    "    def run(self):\n",
    "        model = SentenceTransformer('paraphrase-albert-small-v2')\n",
    "        df = pd.DataFrame(df_read)\n",
    "        paraphrases = util.paraphrase_mining(model, sentences)\n",
    "        model = paraphrases \n",
    "        self.save(model)\n",
    "        # [df['url'], + [', '] + df['pub date']]\n",
    "flow = d6tflow.Workflow()\n",
    "flow.run(ParaphraseTask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b26a3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_read = pd.read_json('sorted_results.json')\n",
    "print(df_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c98007",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flow = d6tflow.Workflow()\n",
    "paraphrases = flow.outputLoad(ParaphraseTask)\n",
    "df = df_read\n",
    "for paraphrase in paraphrases[95:100]:\n",
    "    score, i, j = paraphrase\n",
    "    # 3 duplicates and a conf paper are 0-3\n",
    "    print(f\"{sentences[i]} \\t\\t {sentences[j]}\\n Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020407ab",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Hold on. There appear to be many paraphrased duplicate articles in our ArXiv query results. Does ArXiv know these dupes are there? I'm guessing, \"no\". Do they care? I have no idea. \n",
    "Let's compare the SPECTER results. \n",
    "So, we revisit our first d6tflow task output and gather the papers in the gap between 2018 and July of 2021 where we started our collection. We want to keep these papers separate so that we can fine tune with only the original meta data from the most recent journal articles as planned. Our d6tflow Workflow object makes this very easy.\n",
    "The most recent dataset of the scientific paper data running on alienai/specter2 is SciRepEval which boasts state-of-the-art multi embeddings per document.[2] To update our model we will use Hugging Face Transformers. \n",
    "Before working on the LangChain prompt, we should see more into SPECTER and paraphrase mining. Using SPECTER as a discriminator layer is possibly enough to weed out any copycat work or data example articles that are matched too high and/or are obviously not authentic. \n",
    "\n",
    "**We can ask, \"How do the results relate to previous two years?\". **\n",
    "\n",
    "The following abstract and title text are a very good fit for my query idea in this notebook. The phrase \"machine learning\" is repeated several times across the paraphrase. This is where we want to set up a more permanent file structure or database for the active searches. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2436a03e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This appears to be a real title and abstract https://arxiv.org/pdf/1907.08908.pdf\n",
    "SPECTERTask.semantic_search_papers(title='Techniques for Automated Machine Learning',\n",
    "              abstract='Automated machine learning (AutoML) aims to find optimal machine learning \\\n",
    "                solutions automatically given a machine learning problem. It could release the \\\n",
    "                burden of data scientists from the multifarious manual tuning process and \\\n",
    "                enable the access of domain experts to the off-the-shelf machine learning \\\n",
    "                solutions without extensive experience. In this paper, we review the current \\\n",
    "                developments of AutoML in terms of three categories, automated feature \\\n",
    "                engineering (AutoFE), automated model and hyperparameter learning (AutoMHL), \\\n",
    "                and automated deep learning (AutoDL). State-of-the-art techniques adopted in \\\n",
    "                the three categories are presented, including Bayesian optimization, \\\n",
    "                reinforcement learning, evolutionary algorithm, and gradient-based approaches. \\\n",
    "                We summarize popular AutoML frameworks and conclude with current open \\\n",
    "                challenges of AutoML.') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db0f6f7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The SPECTER sentence transformer just maps a util semantic search. Let's open up SPECTER2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4d2d7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/cmp-lg/9405014.pdf\n",
    "title = 'Classifying Cue Phrases in Text and Speech Using Machine Learning.'\n",
    "abstract = 'Cue phrases may be used in a discourse sense to explicitly signal discourse \\\n",
    "            structure, but also in a sentential sense to convey semantic rather than \\\n",
    "            structural information. This paper explores the use of machine learning for \\\n",
    "            classifying cue phrases as discourse or sentential. Two machine learning \\\n",
    "            programs (Cgrendel and C4.5) are used to induce classification rules from sets \\\n",
    "            of pre-classified cue phrases and their features. Machine learning is shown to \\\n",
    "            be an effective technique for not only automating the generation of \\\n",
    "            classification rules, but also for improving upon previous results.'\n",
    "\n",
    "SPECTERTask.specter2_search_papers(title=f'{title}', abstract=f'{abstract}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8879533",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Wow. The results for the SPECTER example are really different than the paraphrase pair for the same text. SPECTER is trained on citations, not on strict similarity like paraprase-albert-small-v2. We can get more similar hits with paraphrase mining of the SPECTER hit list which is returned when the search papers function is called via the SPECTERTask d6tflow task class. \n",
    "We have whole embeddings for a large span of articles across article publication dates. So, any embedding for a single article can be compared to the entire embedding for the prior two years worth of articles in our query results. \n",
    "There is also the newer alienai/specter2 model which we can see some more specific embedding based results, but it's not going to make it into this tutorial essay.  \n",
    "\n",
    "Seeing the paraphrase miner results reduce the number of total usable query results makes me squeamish as a data scientist. I think most of the duplicates are just twice submitted versions of the same work.  \n",
    "\n",
    "Thankfully, we have SPECTER which is going to return results based on citations rather than a similarity score. The similarity scores are usually quite low which is a good indicator that they are not just plagiarized replicas, but actually cited articles. We definitely want to clean the data for statistical analysis and modeling. This is quite interesting to have found in such a big project centered around science. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764c9eb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recursive searches for paraphrase miner and SPECTER \n",
    "# search string title in sentences and reorder a df by occurances -\n",
    "# return paraphrase miner pairs, return unique SPECTER results for unique items \n",
    "# save data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb285cfa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(paraphrases[:2])\n",
    "print(sentences[0])\n",
    "# sentences is a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f6143",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve and rerank \n",
    "# https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/in_document_search_crossencoder.py#L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a335f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct the prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b556e5f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install langchain openai InstructorEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d10a46",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip freeze>requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ed4aae",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Let's see which articles are the best to summarize the previous two years worth of results with \"machine learning\" in the title. We need to find the hits with the highest cosine similarity to the whole two year embedding. Those will be the most recent articles that are most similar to the collection of results from two years prior. Then we will do the same for the older results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951a3d41",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.docstore.document import Document\n",
    "import openai \n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "from InstructorEmbedding import INSTRUCTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a8799",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = ''\n",
    "docs = ''\n",
    "docs = [Document(page_content=t) for t in texts[:]]\n",
    "text_splitter = CharacterTextSplitter()\n",
    "template_string = '''The occrance of {topic} in title texts for a corpus of data in query results; a recent two year span analysis.'''\n",
    "\n",
    "\n",
    "title_template = PromptTemplate(\n",
    "            template=template_string,\n",
    "            input_variables = ['topic'],\n",
    ")\n",
    "\n",
    "template_string = '''Write an 8 paragraph essay based on this title TITLE: {title}'''\n",
    "\n",
    "abstract_template = PromptTemplate(\n",
    "            template = template_string,\n",
    "            input_variables = ['topic'],\n",
    "            \n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "title_chain = LLMChain(llm=llm, prompt=title_template, verbose=True, output_key='title')\n",
    "summary_chain = LLMChain(llm=llm, prompt=abstract_template, verbose=True, output_key='summary')\n",
    "sequential_chain = SequentialChain(chains=[title_chain, summary_chain], verbose=True, \n",
    "                                    input_variables=['topic'], output_variables=['title', 'summary'])\n",
    "\n",
    "\n",
    "docs = ''\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "chain.run(docs)\n",
    "\n",
    "\n",
    "\n",
    "if prompt:\n",
    "    response = sequential_chain.run(topic=prompt)\n",
    "    print(response, sequential_chain.output_variables)\n",
    "    \n",
    "prompt = f'the main differences in the meta data for {word} within the \\\n",
    "                        last two years as opposed to prior than two years'\n",
    "prompt = '''Summary of two years worth of machine learning meta data ArXiv query results.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add447cc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = INSTRUCTOR('hkunlp/instructor-xl')\n",
    "model = SentenceTransformer(\"hkunlp/instructor-xl\")\n",
    "\n",
    "sentence = \"3D ActionSLAM: wearable person tracking in multi-floor environments\"\n",
    "instruction = \"Represent the Science title:\"\n",
    "embeddings = model.encode([[instruction,sentence]])\n",
    "sims = cosine_similarity(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sentences_a = [['Represent the Science sentence: ','Parton energy loss in QCD matter'], \n",
    "               ['Represent the Financial statement: ','The Federal Reserve on Wednesday raised its benchmark interest rate.']]\n",
    "sentences_b = [['Represent the Science sentence: ','The Chiral Phase Transition in Dissipative Dynamics'],\n",
    "               ['Represent the Financial statement: ','The funds rose less than 0.5 per cent on Friday']]\n",
    "\n",
    "embeddings_a = model.encode(sentences_a)\n",
    "embeddings_b = model.encode(sentences_b)\n",
    "similarities = cosine_similarity(embeddings_a,embeddings_b)\n",
    "print(sims, similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60876b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flow = d6tflow.Workflow()\n",
    "df = flow.outputLoad(SecondSortTask)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "model = INSTRUCTOR('hkunlp/instructor-large')\n",
    "dates = [f\"Date: {date} of Title : {title}\" for date, title in zip(df['date'], df['title'])]\n",
    "titles = [{\"instruction\": f'Represent the Science title: ', \"text\": [f'{title} {date}' for title, date in (zip(df['title'], df['date']))]}]\n",
    "title = str()\n",
    "abstract = str()\n",
    "# We can load a different model of embeddigns without losing current model assignment \n",
    "# titles_embeddings = model.encode(titles)\n",
    "texts_with_instructions = []\n",
    "pairs = [\n",
    "    {\"instruction\": \"Represent the Science sentence for clustering:\", \"text\":  \n",
    "     [f'{title}' for title, abstract in (zip(df['title'], df['abstract']))]},\n",
    "    {\"instruction\": \"Represent the Science abstract for clustering:\", \"text\":  \n",
    "     [f'{abstract}' for title, abstract in (zip(df['title'], df['abstract']))]}\n",
    "]\n",
    "\n",
    "# or pair in pairs:\n",
    "    # TODO fix this loop \n",
    "    # texts_with_instructions.append([pair[\"instruction\"], pair[\"text\"]])\n",
    "# cluster_embeddings = model.encode(texts_with_instructions)\n",
    "\n",
    "# query  = [['Represent the Science question for retrieving supporting documents: ',f'{title}']]\n",
    "# corpus = [['Represent the Science document for retrieval: ', f'{abstract}']]\n",
    "          \n",
    "# query_embeddings = model.encode(query)\n",
    "# corpus_embeddings = model.encode(corpus)\n",
    "# similarities = cosine_similarity(query_embeddings,corpus_embeddings)\n",
    "# retrieved_doc_id = np.argmax(similarities)\n",
    "# print(retrieved_doc_id)\n",
    "# print(similarities)\n",
    "# prepare texts with instructions for titles and abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8031508f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "working_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aead4b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We just made an ad hoc query to an old dataset which returned a result based on relevance to our "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307de78",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from pprint import pprint\n",
    "from transformers import ElectraTokenizer, ElectraForPreTraining\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc29768",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip freeze>requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aec848",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by date and calculate the daily count of titles with 'machine learning'\n",
    "ml_daily = df.groupby('date')['contains_search_string'].sum().reset_index()\n",
    "df['string_counter'] = df['contains_search_string'].cumsum()\n",
    "\n",
    "# Create the plot\n",
    "fig = px.line(df, x='date', y='string_counter', title=f'Count of Titles With \"{word}\" Over Time in Query: {query_words}')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7c65c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "ElectraForPreTraining is a variant of the BERT model but with a different pre-training task. Instead of the masked language modeling task used in BERT, ELECTRA uses a novel pre-training task called replaced token detection. It has a discriminator that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc42ec",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimilarSearchTask():\n",
    "    def requires():\n",
    "        return EmbeddingsTask()\n",
    "    \n",
    "    def run():\n",
    "        \n",
    "        EmbeddingsTask = self.inputLoad()\n",
    "        dataset_file = 'prior_articles.json'\n",
    "        if not os.path.exists(dataset_file):\n",
    "            working_directory()\n",
    "        with open(dataset_file) as fIn:\n",
    "            papers = json.load(fIn)\n",
    "        \n",
    "        # We want to find a real recent article to find out similar articles about \n",
    "        tensor = '/kaggle/working/data/EmbeddingsTask/EmbeddingsTask__99914b932b-data.pkl'\n",
    "\n",
    "        file = pickle.load(tensor)\n",
    "        texts = []\n",
    "        # Create a mapping from index to text\n",
    "        index_to_text = {i: text for i, text in enumerate(texts)}\n",
    "            \n",
    "            \n",
    "        EmbeddingsTask.search_papers('Meta Study Analysis Of Machine Learning', \"This paper is cutting edge and \\\n",
    "                                     has beaten the competition.\", file, papers)\n",
    "        \n",
    "    def nearest_neighbors():\n",
    "        \n",
    "        embeddings = np.array(embeddings)\n",
    "        # Create a NearestNeighbors model\n",
    "        model = NearestNeighbors(n_neighbors=5, algorithm='ball_tree')\n",
    "        model.fit(embeddings)\n",
    "        # Assuming 'new_embedding' is the embedding you want to search for\n",
    "        new_embedding = np.array([...])\n",
    "        # Find the nearest neighbors to your new embedding\n",
    "        distances, indices = model.kneighbors([new_embedding])\n",
    "        # 'indices' will be a 2D array containing the indices of the nearest neighbors\n",
    "        nearest_neighbors = indices[0]\n",
    "        # Use these indices to get the corresponding texts\n",
    "        similar_texts = [index_to_text[i] for i in nearest_neighbors]\n",
    "        print(similar_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae11696",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "working_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096a2189",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The task of autowriting an essay is tricky. How will we be able to assure accuracy of anything? Since alienai/specter2 has more data in its dataset it provides us the freedom to compare our essay results based on different models and data. \n",
    "\n",
    "We can also formulate a difference in our datasets for comparison to understand how much more or less each set has than the other while comparing the final results. Remember, we want the essay to improve upon article information prior to the last two years for the last two years worth of articles. This shouldn't be a prolem for a large language model prompt, e.g. ChatGPT4. However, we don't want to be ignorant of mistakes to correct or improvements to make. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19633c82",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We can now utilize the OpenAI API which has very nice text embeddings for categorizing semantic sentiment similarity, and of course, Chat-GPT4 for a text completion prompt model to write an essay. There are additional cutting edge libraries that we will need in order to deal with the text in our data source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b080ec15",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO gpt-3.5-turbo-16k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c2ca7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO generate a citations list in APA for the analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97123720",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_reset(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21292a50",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "1. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel S. Weld (2020). SPECTER: Document-level Representation Learning using Citation-informed Transformers.\n",
    "[v4]. ArXiv. https://doi.org/10.48550/arXiv.2004.07180\n",
    "\n",
    "2. https://huggingface.co/transformers/v3.0.2/model_doc/auto.html\n",
    "\n",
    "3. Singh, A., D'Arcy, M., Cohan, A., Downey, D., & Feldman, S. (2022). SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. ArXiv, abs/2211.13308.\n",
    "\n",
    "4. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\n",
    "\n",
    "5. OpenAI. (Year). ChatGPT (Month Day version) [Large language model]. https://chat.openai.com\n",
    "\n",
    "6. https://d6tflow.readthedocs.io/en/latest/tasks.html#save-output-data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 185.288155,
   "end_time": "2023-07-27T03:54:54.863113",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-27T03:51:49.574958",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}