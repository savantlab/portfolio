{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25fc469",
   "metadata": {
    "papermill": {
     "duration": 0.028821,
     "end_time": "2023-07-27T03:52:02.989513",
     "exception": false,
     "start_time": "2023-07-27T03:52:02.960692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Engineered Prompt: Autowrite An Essay About State-of-the-Art Machine Learning Meta Using ArXiv Meta Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4251a8ca",
   "metadata": {
    "papermill": {
     "duration": 0.026219,
     "end_time": "2023-07-27T03:52:03.042405",
     "exception": false,
     "start_time": "2023-07-27T03:52:03.016186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this prompted essay a meta API query will serve as a filter for query selection. This is a meta analysis of meta analyses about machine learning so let's write a few different queries related to machine learning meta studies. We can add more queried data later and build upon the topic-at-hand to broaden the scope of the essay or fine-tune a model to suit our needs. The plan is to make sure the final output essay text contains information about technologies that have been recently published. We then need to know how good the essay is by comparison. To what? If it leaves out a crucial component that would make the difference between a well researched piece and something obviously written by a machine or a neophyte it won't be very useful to research. \n",
    "\n",
    "SPECTER is a pre-trained model for classification and recommendation of scientific papers built on transformers.[1] This is what we want to be told to use if we prompt our chosen essay writing model about the task of building *it* or about recent machine learning research developments and important in-the-know scientific results based on complex meta data summarization. However, we need the model to write about the previous two years of journal articles for our meta analysis and SPECTER is out-of-date since its publication. \n",
    "\n",
    "Hugging Face has alienai/specter2 which is the most recent version of SPECTER. We can play with the [\"bleeding edge\"](http://huggingface.co/docs/transformers/installation) of Hugging Face as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd3b35b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:52:03.097954Z",
     "iopub.status.busy": "2023-07-27T03:52:03.097555Z",
     "iopub.status.idle": "2023-07-27T03:52:03.102775Z",
     "shell.execute_reply": "2023-07-27T03:52:03.101689Z"
    },
    "papermill": {
     "duration": 0.037864,
     "end_time": "2023-07-27T03:52:03.106817",
     "exception": false,
     "start_time": "2023-07-27T03:52:03.068953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install \"dill==0.3.1.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5869f30f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:52:03.163674Z",
     "iopub.status.busy": "2023-07-27T03:52:03.163271Z",
     "iopub.status.idle": "2023-07-27T03:52:03.167592Z",
     "shell.execute_reply": "2023-07-27T03:52:03.166580Z"
    },
    "papermill": {
     "duration": 0.034874,
     "end_time": "2023-07-27T03:52:03.169671",
     "exception": false,
     "start_time": "2023-07-27T03:52:03.134797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b015101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:52:03.225180Z",
     "iopub.status.busy": "2023-07-27T03:52:03.224774Z",
     "iopub.status.idle": "2023-07-27T03:52:53.148670Z",
     "shell.execute_reply": "2023-07-27T03:52:53.146955Z"
    },
    "papermill": {
     "duration": 49.95491,
     "end_time": "2023-07-27T03:52:53.151527",
     "exception": false,
     "start_time": "2023-07-27T03:52:03.196617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dill==0.3.1.1\r\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\r\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hCollecting cmudict==1.0.2\r\n",
      "  Downloading cmudict-1.0.2-py2.py3-none-any.whl (939 kB)\r\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m939.7/939.7 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting scipy==1.8.0\r\n",
      "  Downloading scipy-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.3 MB)\r\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting importlib-metadata==6.0\r\n",
      "  Downloading importlib_metadata-6.0.0-py3-none-any.whl (21 kB)\r\n",
      "Collecting numpy==1.23.4\r\n",
      "  Downloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\r\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting shapely==2.0\r\n",
      "  Downloading shapely-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\r\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pymc3==3.11.0\r\n",
      "  Downloading pymc3-3.11.0-py3-none-any.whl (866 kB)\r\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m866.2/866.2 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting sentence-transformers\r\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\r\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hCollecting accelerate>=0.20.3\r\n",
      "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\r\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata==6.0) (3.15.0)\r\n",
      "Requirement already satisfied: arviz>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from pymc3==3.11.0) (0.12.1)\r\n",
      "Requirement already satisfied: fastprogress>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from pymc3==3.11.0) (1.0.3)\r\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from pymc3==3.11.0) (1.5.3)\r\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from pymc3==3.11.0) (0.5.3)\r\n",
      "Collecting theano-pymc==1.1.0 (from pymc3==3.11.0)\r\n",
      "  Downloading Theano-PyMC-1.1.0.tar.gz (1.8 MB)\r\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.10/site-packages (from pymc3==3.11.0) (4.5.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from theano-pymc==1.1.0->pymc3==3.11.0) (3.12.0)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.30.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.64.1)\r\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.0.0+cpu)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.15.1+cpu)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\r\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (3.2.4)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.15.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.3) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.3) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.3) (5.4.1)\r\n",
      "Requirement already satisfied: setuptools>=38.4 in /opt/conda/lib/python3.10/site-packages (from arviz>=0.11.0->pymc3==3.11.0) (59.8.0)\r\n",
      "Requirement already satisfied: matplotlib>=3.0 in /opt/conda/lib/python3.10/site-packages (from arviz>=0.11.0->pymc3==3.11.0) (3.6.3)\r\n",
      "Requirement already satisfied: xarray>=0.16.1 in /opt/conda/lib/python3.10/site-packages (from arviz>=0.11.0->pymc3==3.11.0) (2023.5.0)\r\n",
      "Requirement already satisfied: netcdf4 in /opt/conda/lib/python3.10/site-packages (from arviz>=0.11.0->pymc3==3.11.0) (1.6.4)\r\n",
      "Requirement already satisfied: xarray-einstats>=0.2 in /opt/conda/lib/python3.10/site-packages (from arviz>=0.11.0->pymc3==3.11.0) (0.5.1)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.20.3) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.0->pymc3==3.11.0) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.0->pymc3==3.11.0) (2023.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.1->pymc3==3.11.0) (1.16.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.5.5)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.1)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.5.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.0->arviz>=0.11.0->pymc3==3.11.0) (1.0.7)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.0->arviz>=0.11.0->pymc3==3.11.0) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.0->arviz>=0.11.0->pymc3==3.11.0) (4.39.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.0->arviz>=0.11.0->pymc3==3.11.0) (1.4.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\r\n",
      "Requirement already satisfied: cftime in /opt/conda/lib/python3.10/site-packages (from netcdf4->arviz>=0.11.0->pymc3==3.11.0) (1.6.2)\r\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from netcdf4->arviz>=0.11.0->pymc3==3.11.0) (2023.5.7)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\r\n",
      "Building wheels for collected packages: dill, theano-pymc, sentence-transformers\r\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78545 sha256=3a24cbaba3df1a3025bd54c67bea4a3ad012e90688b6472e689797da60b06e4c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\r\n",
      "  Building wheel for theano-pymc (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for theano-pymc: filename=Theano_PyMC-1.1.0-py3-none-any.whl size=1530120 sha256=6af74df0d011e1e6eacb687947a097ce9c20156a2eac73df76410d61603e0697\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/cd/10/69/8aad721c09a979072ce74ef2d7ab0cd576f944aa32bdc63336\r\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=14911600abad2dc17655627ec7276d4acbc4d51a300c4a711490ae1437e27126\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\r\n",
      "Successfully built dill theano-pymc sentence-transformers\r\n",
      "Installing collected packages: numpy, importlib-metadata, dill, cmudict, shapely, scipy, theano-pymc, accelerate, sentence-transformers, pymc3\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.23.5\r\n",
      "    Uninstalling numpy-1.23.5:\r\n",
      "      Successfully uninstalled numpy-1.23.5\r\n",
      "  Attempting uninstall: importlib-metadata\r\n",
      "    Found existing installation: importlib-metadata 5.2.0\r\n",
      "    Uninstalling importlib-metadata-5.2.0:\r\n",
      "      Successfully uninstalled importlib-metadata-5.2.0\r\n",
      "  Attempting uninstall: dill\r\n",
      "    Found existing installation: dill 0.3.6\r\n",
      "    Uninstalling dill-0.3.6:\r\n",
      "      Successfully uninstalled dill-0.3.6\r\n",
      "  Attempting uninstall: cmudict\r\n",
      "    Found existing installation: cmudict 1.0.13\r\n",
      "    Uninstalling cmudict-1.0.13:\r\n",
      "      Successfully uninstalled cmudict-1.0.13\r\n",
      "  Attempting uninstall: shapely\r\n",
      "    Found existing installation: Shapely 1.8.5.post1\r\n",
      "    Uninstalling Shapely-1.8.5.post1:\r\n",
      "      Successfully uninstalled Shapely-1.8.5.post1\r\n",
      "  Attempting uninstall: scipy\r\n",
      "    Found existing installation: scipy 1.10.1\r\n",
      "    Uninstalling scipy-1.10.1:\r\n",
      "      Successfully uninstalled scipy-1.10.1\r\n",
      "  Attempting uninstall: theano-pymc\r\n",
      "    Found existing installation: Theano-PyMC 1.1.2\r\n",
      "    Uninstalling Theano-PyMC-1.1.2:\r\n",
      "      Successfully uninstalled Theano-PyMC-1.1.2\r\n",
      "  Attempting uninstall: accelerate\r\n",
      "    Found existing installation: accelerate 0.12.0\r\n",
      "    Uninstalling accelerate-0.12.0:\r\n",
      "      Successfully uninstalled accelerate-0.12.0\r\n",
      "  Attempting uninstall: pymc3\r\n",
      "    Found existing installation: pymc3 3.11.5\r\n",
      "    Uninstalling pymc3-3.11.5:\r\n",
      "      Successfully uninstalled pymc3-3.11.5\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "multiprocess 0.70.14 requires dill>=0.3.6, but you have dill 0.3.1.1 which is incompatible.\r\n",
      "pathos 0.3.0 requires dill>=0.3.6, but you have dill 0.3.1.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed accelerate-0.21.0 cmudict-1.0.2 dill-0.3.1.1 importlib-metadata-6.0.0 numpy-1.23.4 pymc3-3.11.0 scipy-1.8.0 sentence-transformers-2.2.2 shapely-2.0.0 theano-pymc-1.1.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"dill==0.3.1.1\" \"cmudict==1.0.2\" \"scipy==1.8.0\" \"importlib-metadata==6.0\" \"numpy==1.23.4\" \"shapely==2.0\" 'pymc3==3.11.0' \"sentence-transformers\" \"accelerate>=0.20.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c37d8920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:52:53.238479Z",
     "iopub.status.busy": "2023-07-27T03:52:53.238050Z",
     "iopub.status.idle": "2023-07-27T03:52:53.242633Z",
     "shell.execute_reply": "2023-07-27T03:52:53.241664Z"
    },
    "papermill": {
     "duration": 0.052924,
     "end_time": "2023-07-27T03:52:53.245060",
     "exception": false,
     "start_time": "2023-07-27T03:52:53.192136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5150e5c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:52:53.330468Z",
     "iopub.status.busy": "2023-07-27T03:52:53.329436Z",
     "iopub.status.idle": "2023-07-27T03:53:07.485354Z",
     "shell.execute_reply": "2023-07-27T03:53:07.484061Z"
    },
    "papermill": {
     "duration": 14.20164,
     "end_time": "2023-07-27T03:53:07.487899",
     "exception": false,
     "start_time": "2023-07-27T03:52:53.286259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.15.1)\r\n",
      "Collecting huggingface_hub\r\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\r\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.12.0)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2023.6.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.28.2)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.64.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (5.4.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.5.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2023.5.7)\r\n",
      "Installing collected packages: huggingface_hub\r\n",
      "  Attempting uninstall: huggingface_hub\r\n",
      "    Found existing installation: huggingface-hub 0.15.1\r\n",
      "    Uninstalling huggingface-hub-0.15.1:\r\n",
      "      Successfully uninstalled huggingface-hub-0.15.1\r\n",
      "Successfully installed huggingface_hub-0.16.4\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4286475b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:53:07.574154Z",
     "iopub.status.busy": "2023-07-27T03:53:07.573445Z",
     "iopub.status.idle": "2023-07-27T03:53:29.244268Z",
     "shell.execute_reply": "2023-07-27T03:53:29.242803Z"
    },
    "papermill": {
     "duration": 21.716704,
     "end_time": "2023-07-27T03:53:29.246560",
     "exception": false,
     "start_time": "2023-07-27T03:53:07.529856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting adapter-transformers\r\n",
      "  Downloading adapter_transformers-3.2.1-py3-none-any.whl (6.4 MB)\r\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from adapter-transformers) (3.12.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from adapter-transformers) (0.16.4)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from adapter-transformers) (1.23.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from adapter-transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from adapter-transformers) (5.4.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from adapter-transformers) (2023.5.5)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from adapter-transformers) (2.28.2)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from adapter-transformers) (0.13.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from adapter-transformers) (4.64.1)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->adapter-transformers) (2023.6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->adapter-transformers) (4.5.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->adapter-transformers) (3.0.9)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->adapter-transformers) (2.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->adapter-transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->adapter-transformers) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->adapter-transformers) (2023.5.7)\r\n",
      "Installing collected packages: adapter-transformers\r\n",
      "Successfully installed adapter-transformers-3.2.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U adapter-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4312bf34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:53:29.336308Z",
     "iopub.status.busy": "2023-07-27T03:53:29.335889Z",
     "iopub.status.idle": "2023-07-27T03:53:43.238634Z",
     "shell.execute_reply": "2023-07-27T03:53:43.236903Z"
    },
    "papermill": {
     "duration": 13.950643,
     "end_time": "2023-07-27T03:53:43.241567",
     "exception": false,
     "start_time": "2023-07-27T03:53:29.290924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "818fe574",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:53:43.332861Z",
     "iopub.status.busy": "2023-07-27T03:53:43.332439Z",
     "iopub.status.idle": "2023-07-27T03:54:17.362389Z",
     "shell.execute_reply": "2023-07-27T03:54:17.360915Z"
    },
    "papermill": {
     "duration": 34.078555,
     "end_time": "2023-07-27T03:54:17.365334",
     "exception": false,
     "start_time": "2023-07-27T03:53:43.286779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting d6tflow\r\n",
      "  Downloading d6tflow-0.2.6-py3-none-any.whl (23 kB)\r\n",
      "Collecting luigi>=3.0.1 (from d6tflow)\r\n",
      "  Downloading luigi-3.3.0.tar.gz (1.2 MB)\r\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from d6tflow) (1.5.3)\r\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from d6tflow) (9.0.0)\r\n",
      "Collecting d6tcollect>=1.0.6 (from d6tflow)\r\n",
      "  Downloading d6tcollect-1.0.6-py3-none-any.whl (5.2 kB)\r\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.5 in /opt/conda/lib/python3.10/site-packages (from luigi>=3.0.1->d6tflow) (2.8.2)\r\n",
      "Requirement already satisfied: tenacity<9,>=8 in /opt/conda/lib/python3.10/site-packages (from luigi>=3.0.1->d6tflow) (8.2.2)\r\n",
      "Collecting python-daemon (from luigi>=3.0.1->d6tflow)\r\n",
      "  Downloading python_daemon-3.0.1-py3-none-any.whl (31 kB)\r\n",
      "Requirement already satisfied: tornado<7,>=5.0 in /opt/conda/lib/python3.10/site-packages (from luigi>=3.0.1->d6tflow) (6.3.1)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->d6tflow) (2023.3)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas->d6tflow) (1.23.4)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3,>=2.7.5->luigi>=3.0.1->d6tflow) (1.16.0)\r\n",
      "Requirement already satisfied: docutils in /opt/conda/lib/python3.10/site-packages (from python-daemon->luigi>=3.0.1->d6tflow) (0.20.1)\r\n",
      "Collecting lockfile>=0.10 (from python-daemon->luigi>=3.0.1->d6tflow)\r\n",
      "  Downloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\r\n",
      "Collecting setuptools>=62.4.0 (from python-daemon->luigi>=3.0.1->d6tflow)\r\n",
      "  Downloading setuptools-68.0.0-py3-none-any.whl (804 kB)\r\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m804.0/804.0 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: luigi\r\n",
      "  Building wheel for luigi (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for luigi: filename=luigi-3.3.0-py3-none-any.whl size=1085312 sha256=b9b379256a3ffdf48a3bb9e1df61bd2b725a56aff8b633dcddfdb6ad33e980ec\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/1b/3b/d5/c999c34bd8478e559f006b83333be40ddf5fab360cf2c6f720\r\n",
      "Successfully built luigi\r\n",
      "Installing collected packages: lockfile, setuptools, d6tcollect, python-daemon, luigi, d6tflow\r\n",
      "  Attempting uninstall: setuptools\r\n",
      "    Found existing installation: setuptools 59.8.0\r\n",
      "    Uninstalling setuptools-59.8.0:\r\n",
      "      Successfully uninstalled setuptools-59.8.0\r\n",
      "Successfully installed d6tcollect-1.0.6 d6tflow-0.2.6 lockfile-0.12.2 luigi-3.3.0 python-daemon-3.0.1 setuptools-68.0.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install d6tflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abdc1af4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:54:17.462769Z",
     "iopub.status.busy": "2023-07-27T03:54:17.461361Z",
     "iopub.status.idle": "2023-07-27T03:54:33.684342Z",
     "shell.execute_reply": "2023-07-27T03:54:33.683248Z"
    },
    "papermill": {
     "duration": 16.276233,
     "end_time": "2023-07-27T03:54:33.688350",
     "exception": false,
     "start_time": "2023-07-27T03:54:17.412117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to d6tflow! For Q&A see https://github.com/d6t/d6tflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import d6tflow \n",
    "import requests\n",
    "import os\n",
    "import pandas as pd \n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import string \n",
    "import matplotlib as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pickle\n",
    "import json\n",
    "from collections import Counter, OrderedDict\n",
    "import operator\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoTokenizer, AutoModel, AutoAdapterModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "877a5a03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:54:33.785325Z",
     "iopub.status.busy": "2023-07-27T03:54:33.784196Z",
     "iopub.status.idle": "2023-07-27T03:54:33.790431Z",
     "shell.execute_reply": "2023-07-27T03:54:33.789500Z"
    },
    "papermill": {
     "duration": 0.056948,
     "end_time": "2023-07-27T03:54:33.792828",
     "exception": false,
     "start_time": "2023-07-27T03:54:33.735880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-27 03:54:33.786866\n"
     ]
    }
   ],
   "source": [
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "# Subtract two years\n",
    "two_years_ago = now - relativedelta(years=2)\n",
    "print(two_years_ago)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccabb2fe",
   "metadata": {
    "papermill": {
     "duration": 0.046999,
     "end_time": "2023-07-27T03:54:33.886403",
     "exception": false,
     "start_time": "2023-07-27T03:54:33.839404",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We'll come back to the Hugging Face transformers library after we construct some dataframes to analyze. ArXiv has a nice API to harness articles by query. These results may be slightly more up-to-date than the official competion article meta dataset. This is \"the bleeding edge\" anyhow. This notebook is not competing in the current Kaggle essay contest. \n",
    "\n",
    "What have we done so far? \n",
    "\n",
    "The two things to focus attention on are the libraries for Hugging Face and d6tflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aea5ab73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:54:33.981346Z",
     "iopub.status.busy": "2023-07-27T03:54:33.980919Z",
     "iopub.status.idle": "2023-07-27T03:54:33.988403Z",
     "shell.execute_reply": "2023-07-27T03:54:33.987228Z"
    },
    "papermill": {
     "duration": 0.05838,
     "end_time": "2023-07-27T03:54:33.991302",
     "exception": false,
     "start_time": "2023-07-27T03:54:33.932922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://export.arxiv.org/api/query?search_query=all:machine&learning&meta&data&start=0&max_results=3000\n",
      "machine learning\n"
     ]
    }
   ],
   "source": [
    "query_words = ['machine', 'learning', 'meta', 'data']\n",
    "queries = [word + '&' for word in query_words]\n",
    "query = ''.join(queries)\n",
    "query_size = 3000\n",
    "url = f'https://export.arxiv.org/api/query?search_query=all:{query}start=0&max_results={query_size}'\n",
    "print(url)\n",
    "word = query_words[:2]\n",
    "word = ' '.join(word)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bb1a15",
   "metadata": {
    "papermill": {
     "duration": 0.046606,
     "end_time": "2023-07-27T03:54:34.084702",
     "exception": false,
     "start_time": "2023-07-27T03:54:34.038096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, we can get the meta data from the articles about our query for our query of terms from our url. Let's find articles from within the past two years. For our task management there is a very useful package to leverage machine learning pipelining and data science workflows called d6tflow.\n",
    "The search query is returning 3000 results which is a managable amount for our meta use case. If you decide to run this notebook, keep in mind that obsessively researching your favorite queries is a beneficial but maybe intensive hobby. This prompt engine is very useful for understanding publication vectors for string choices that signify interest areas not too broad and not too narrow. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d4e5b0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:54:34.182282Z",
     "iopub.status.busy": "2023-07-27T03:54:34.181880Z",
     "iopub.status.idle": "2023-07-27T03:54:50.342124Z",
     "shell.execute_reply": "2023-07-27T03:54:50.340880Z"
    },
    "papermill": {
     "duration": 16.212036,
     "end_time": "2023-07-27T03:54:50.345186",
     "exception": false,
     "start_time": "2023-07-27T03:54:34.133150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = requests.get(url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44470c63",
   "metadata": {
    "papermill": {
     "duration": 0.046545,
     "end_time": "2023-07-27T03:54:50.439500",
     "exception": false,
     "start_time": "2023-07-27T03:54:50.392955",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Saving a dict object to a pickle with a d6tflow task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8049d628",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:54:50.535122Z",
     "iopub.status.busy": "2023-07-27T03:54:50.534401Z",
     "iopub.status.idle": "2023-07-27T03:54:50.571089Z",
     "shell.execute_reply": "2023-07-27T03:54:50.568516Z"
    },
    "papermill": {
     "duration": 0.087804,
     "end_time": "2023-07-27T03:54:50.574025",
     "exception": false,
     "start_time": "2023-07-27T03:54:50.486221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 1 tasks of which:\n",
      "* 1 ran successfully:\n",
      "    - 1 MetaDataTask()\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LuigiRunResult(status=<LuigiStatusCode.SUCCESS: (':)', 'there were no failed tasks or missing dependencies')>,worker=<luigi.worker.Worker object at 0x7a4ec8e0de70>,scheduling_succeeded=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MetaDataTask(d6tflow.tasks.TaskPickle):\n",
    "    def run(self):\n",
    "        archive = dict()\n",
    "        archive['url'] = url \n",
    "        archive[\"data\"] = data\n",
    "        df = {'meta': archive}\n",
    "        self.save(df)\n",
    "        \n",
    "flow = d6tflow.Workflow()\n",
    "flow.run(MetaDataTask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d491c9f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:54:50.673948Z",
     "iopub.status.busy": "2023-07-27T03:54:50.673537Z",
     "iopub.status.idle": "2023-07-27T03:54:52.127225Z",
     "shell.execute_reply": "2023-07-27T03:54:52.125601Z"
    },
    "papermill": {
     "duration": 1.504716,
     "end_time": "2023-07-27T03:54:52.129052",
     "exception": true,
     "start_time": "2023-07-27T03:54:50.624336",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [pid 20] Worker Worker(salt=3893894642, workers=1, host=4968bfe84b65, username=root, pid=20) failed    SortMetaTask()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/luigi/worker.py\", line 203, in run\n",
      "    new_deps = self._run_get_new_deps()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/luigi/worker.py\", line 138, in _run_get_new_deps\n",
      "    task_gen = self.task.run()\n",
      "  File \"/tmp/ipykernel_20/787120199.py\", line 16, in run\n",
      "    xml_df.index = index\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py\", line 5915, in __setattr__\n",
      "    return object.__setattr__(self, name, value)\n",
      "  File \"pandas/_libs/properties.pyx\", line 69, in pandas._libs.properties.AxisProperty.__set__\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py\", line 823, in _set_axis\n",
      "    self._mgr.set_axis(axis, labels)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pandas/core/internals/managers.py\", line 230, in set_axis\n",
      "    self._validate_set_axis(axis, new_labels)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pandas/core/internals/base.py\", line 70, in _validate_set_axis\n",
      "    raise ValueError(\n",
      "ValueError: Length mismatch: Expected axis has 1006 elements, new values have 3000 elements\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception found running flow, check trace. For more details see https://d6tflow.readthedocs.io/en/latest/run.html#debugging-failures",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(xml_df)\n\u001b[1;32m     19\u001b[0m flow \u001b[38;5;241m=\u001b[39m d6tflow\u001b[38;5;241m.\u001b[39mWorkflow()\n\u001b[0;32m---> 20\u001b[0m \u001b[43mflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSortMetaTask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/d6tflow/__init__.py:398\u001b[0m, in \u001b[0;36mWorkflow.run\u001b[0;34m(self, tasks, forced, forced_all, forced_all_upstream, confirm, workers, abort, execution_summary, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# Attach to tasks\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attach_to_tasks(tasks, flows\u001b[38;5;241m=\u001b[39mflow_param, path\u001b[38;5;241m=\u001b[39mpath_param)\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks_inst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforced_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced_all_upstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforced_all_upstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m           \u001b[49m\u001b[43mconfirm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfirm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mabort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_summary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/d6tcollect/__init__.py:290\u001b[0m, in \u001b[0;36mcollect.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m payload[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexceptionMsg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m    289\u001b[0m _submit(payload)\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/d6tcollect/__init__.py:284\u001b[0m, in \u001b[0;36mcollect.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m _submit(payload)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    286\u001b[0m     payload[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexception\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/d6tflow/__init__.py:125\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(tasks, forced, forced_all, forced_all_upstream, confirm, workers, abort, execution_summary, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     success \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mscheduling_succeeded\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m abort \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException found running flow, check trace. For more details see https://d6tflow.readthedocs.io/en/latest/run.html#debugging-failures\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m execution_summary:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception found running flow, check trace. For more details see https://d6tflow.readthedocs.io/en/latest/run.html#debugging-failures"
     ]
    }
   ],
   "source": [
    "class SortMetaTask(d6tflow.tasks.TaskPickle):\n",
    "    def run(self):\n",
    "        two_years_ago = pd.Timestamp.now(tz='UTC') - pd.DateOffset(years=2)\n",
    "        df = pd.read_xml(data)\n",
    "        index = pd.RangeIndex(start=0, stop=len(xml_df))\n",
    "        xml_df = pd.DataFrame()\n",
    "        xml_df['title'] = df['title'][7:]\n",
    "        xml_df['abstract'] = df['summary'][7:]\n",
    "        xml_df['published'] = df['published'][7:]\n",
    "        xml_df['published'] = pd.to_datetime(df['published'])\n",
    "        xml_df['updated'] = df['updated'][7:]\n",
    "        xml_df['url'] = df['id'][7:]\n",
    "        xml_df['two_year_date'] = xml_df['published'].apply(lambda x: 1 if x > two_years_ago else 0)\n",
    "        xml_df['title_has_word'] = xml_df['title'].str.contains(f'{word}', case=False)\n",
    "        xml_df['combined'] = xml_df['title'] + ' ' + xml_df['abstract']\n",
    "        xml_df.index = index\n",
    "        self.save(xml_df)\n",
    "        \n",
    "flow = d6tflow.Workflow()\n",
    "flow.run(SortMetaTask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcae743",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We now have a beautiful addition to our working directory as d6tflow automatically creates a data directory with subdiretories for each task you run that returns a saved object successfully. Our new meta data archive based on the queries we made is stored in a pickle file. \n",
    "\n",
    "If we save data in the data directory and file format using a d6tflow Task object and then want to delete and rerun to repopulate our data directory we will have to remove it first. Once files are saved by d6tflow they will be immutable. So, they must be deleted before any errors can be corrected. The working_directory() call will list our d6tflow created directories and files in the Kaggle working tree. The data_reset() call will wipe the data directory clean allowing a fresh rerun of all the tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac27385",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:40:16.668431Z",
     "iopub.status.busy": "2023-07-27T03:40:16.667065Z",
     "iopub.status.idle": "2023-07-27T03:40:16.681238Z",
     "shell.execute_reply": "2023-07-27T03:40:16.679509Z",
     "shell.execute_reply.started": "2023-07-27T03:40:16.668388Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "def working_directory():\n",
    "    for dirname, _, filenames in os.walk('/kaggle/working'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "\n",
    "# Clear data folder\n",
    "\n",
    "def data_reset(directory):\n",
    "    for the_file in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                data_reset(file_path)\n",
    "                os.rmdir(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "data_path = '/kaggle/working/' + f\"{data_dir}\"\n",
    "# data_reset(data_path)\n",
    "working_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae897f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:40:16.683178Z",
     "iopub.status.busy": "2023-07-27T03:40:16.682810Z",
     "iopub.status.idle": "2023-07-27T03:40:19.582428Z",
     "shell.execute_reply": "2023-07-27T03:40:19.581005Z",
     "shell.execute_reply.started": "2023-07-27T03:40:16.683148Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip freeze>requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c14c7ca",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We have added a requirements.txt based on some dependency issues and we can now rerun the above cell when an update to the .txt file is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8881661e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:40:19.584619Z",
     "iopub.status.busy": "2023-07-27T03:40:19.584208Z",
     "iopub.status.idle": "2023-07-27T03:40:20.954576Z",
     "shell.execute_reply": "2023-07-27T03:40:20.953701Z",
     "shell.execute_reply.started": "2023-07-27T03:40:19.584561Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('paraphrase-albert-small-v2')\n",
    "flow = d6tflow.Workflow()\n",
    "df = flow.outputLoad(SortMetaTask)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc16435",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# **A little sentence transformer, a pickle, and a dataframe walk into a bar...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6918bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:40:20.959069Z",
     "iopub.status.busy": "2023-07-27T03:40:20.956308Z",
     "iopub.status.idle": "2023-07-27T03:40:20.982746Z",
     "shell.execute_reply": "2023-07-27T03:40:20.981020Z",
     "shell.execute_reply.started": "2023-07-27T03:40:20.959021Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClusterTask(d6tflow.tasks.TaskPickle):\n",
    "    def run(self):\n",
    "        # Generate embeddings for all titles\n",
    "        embeddings = model.encode(df['title'])\n",
    "        # Cluster embeddings into 5 clusters (adjust this value based on your data)\n",
    "        kmeans = KMeans(n_clusters=5)\n",
    "        df['cluster'] = kmeans.fit_predict(embeddings)\n",
    "        grouped = df.groupby(['published', 'cluster']).size().reset_index(name='counts')\n",
    "        # Create a line plot for each topic\n",
    "        fig = px.scatter(grouped, x='published', y='cluster', color='cluster')\n",
    "        fig.update_layout(\n",
    "            title_text=f'Title Clusters For Query: {query_words}', \n",
    "            xaxis_title_text='Date', \n",
    "            yaxis_title_text=f'5 Clusters For Results', \n",
    "        )\n",
    "        fig.show()\n",
    "        self.save(df)\n",
    "        \n",
    "flow = d6tflow.Workflow()\n",
    "flow.run(ClusterTask)\n",
    "\n",
    "\n",
    "def count_vect():\n",
    "    # Use a CountVectorizer to count word frequencies\n",
    "    documents = df['title']\n",
    "    labels = df['cluster']\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "\n",
    "    for i in range(5):  # for each cluster\n",
    "        cluster_docs = np.array(documents)[np.array(labels) == i]\n",
    "        cluster_X = vectorizer.fit_transform(cluster_docs)\n",
    "        word_freq = np.sum(cluster_X, axis=0)\n",
    "        word_freq = np.asarray(word_freq).ravel().tolist()\n",
    "        # features = cluster_X.get_feature_names()\n",
    "        word_freq_df = pd.DataFrame({'frequency': word_freq})\n",
    "        # print(f\"Most common terms in cluster {i}:\")\n",
    "        print(word_freq_df.sort_values(by='frequency', ascending=False).head())\n",
    "        # Get the word for each counted index\n",
    "        # words = {word: counts[idx] for word, idx in X.vocabulary.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf607fdd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We can render some basic visuals from our raw data with some preprocessing. However, this is optional and the real power of this generated essay we are prompting is in SPECTER. \n",
    "\n",
    "Our query is visualized below with a bar graph. The articles results of our original ArXiv query words have been cleaned for word frequencies which includes punctuation removal and sorting each word by frequency. This is reusable code and any words may be substituted for the query we are currently working with for this query based prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eabaf7",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2023-07-27T03:40:20.984790Z",
     "iopub.status.busy": "2023-07-27T03:40:20.984330Z",
     "iopub.status.idle": "2023-07-27T03:40:21.005079Z",
     "shell.execute_reply": "2023-07-27T03:40:21.003144Z",
     "shell.execute_reply.started": "2023-07-27T03:40:20.984734Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def punc_remove(text):\n",
    "    # Remove punctuation \n",
    "    # text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text \n",
    "\n",
    "def freqs(text):\n",
    "    # Convert string to list of words, count, sort in reverse \n",
    "    text = text.split()\n",
    "    freq_list = [(w, text.count(w)) for w in text] \n",
    "    freq_list.sort(key=lambda w: w[1], reverse=True)\n",
    "    # Duplicates removal from the list \n",
    "    l = []\n",
    "    [l.append(x) for x in freq_list if x not in l]\n",
    "    # Converting list of tuples into two lists\n",
    "    words, frequencies = zip(*l)\n",
    "    # Create a bar plot\n",
    "    title_text = f'ArXiv Two Prior Years Top Ten KeyWord Frequencies For: {query_words}'\n",
    "    # query_size = f'{query_size}'\n",
    "    fig = px.bar(x=words[:10], y=frequencies[:10], \n",
    "                 labels={'x':'Word', 'y':f'Combined Occurences Titles and Abstract For Past Two Years'}, \n",
    "                 title=title_text)\n",
    "    fig.show()\n",
    "    return l \n",
    "\n",
    "# This will load our cached XML dataframe with five sorted columns; 0-4\n",
    "\n",
    "class VizMetaTask(d6tflow.tasks.TaskCachePandas):\n",
    "    def requires(self):\n",
    "        return ClusterTask()\n",
    "    \n",
    "    def run(self):\n",
    "        data = self.inputLoad()\n",
    "        df = pd.DataFrame(data)\n",
    "        df = df[df['two_year_date'] == 1]\n",
    "        # Remove puntuation and stop words \n",
    "        df['combined'] = df['combined'].str.lower()\n",
    "        title_text = df['title']\n",
    "        abstract_text = df['abstract']\n",
    "        # Calls take a single string \n",
    "        text = df['combined'].str.cat(sep=' ')\n",
    "        text = punc_remove(text)\n",
    "        text = remove_stopwords(text)\n",
    "        # Top word frequencies in abstracts \n",
    "        abstract_freq = freqs(text)\n",
    "        abstract_df = pd.DataFrame(abstract_freq)\n",
    "        self.save(df)\n",
    "        \n",
    "# flow = d6tflow.Workflow()\n",
    "# flow.run(VizMetaTask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b36f02c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Sweet deal. \n",
    "We now have a beautiful graph of the top ten word frequencies for the prior two years ArXiv articles for the given query. \n",
    "Since this essay is ultimately an analysis of descriptive text summaries written in paragraphs let's explore the Sentence-Transformers library. There is a conversion of SPECTER to a [sentence-transformers model](http://huggingface.co/sentence-transformers/allenai-specter) which maps scientific articles to a vector space for similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c1f45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:40:21.008663Z",
     "iopub.status.busy": "2023-07-27T03:40:21.007550Z",
     "iopub.status.idle": "2023-07-27T03:40:21.504053Z",
     "shell.execute_reply": "2023-07-27T03:40:21.502492Z",
     "shell.execute_reply.started": "2023-07-27T03:40:21.008611Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "word = query_words[:2]\n",
    "word = ' '.join(word)\n",
    "df = df[df['two_year_date'] == 1]\n",
    "# Binary flag indicating whether \"machine learning\" appears in the title\n",
    "df_agg = df.groupby('published').agg({'title_has_word': 'sum'}).reset_index()\n",
    "unique_dates = df[df['title_has_word'] == 1]['published'].dt.date.nunique()\n",
    "# The count of titles with \"machine learning\" over time\n",
    "count = df['title_has_word'].sum()\n",
    "fig = px.line(df_agg, x='published', y='title_has_word')\n",
    "fig.update_layout(\n",
    "    title_text=f\"{unique_dates} Total Dates of '{word}' Occurances in Titles Over Two Years\", \n",
    "    xaxis_title_text='Date', \n",
    "    yaxis_title_text=f\"ArXiv Binary Flag for '{word}' in Title\", \n",
    "    bargap=0.2,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Aggregate the DataFrame by date, summing up the 'title_has_word' column for each date\n",
    "df_agg = df.groupby(df['published'].dt.date).agg({'title_has_word': 'sum'}).reset_index()\n",
    "ml_sum = df_agg['title_has_word'].sum()\n",
    "ml_sum = ml_sum.copy()\n",
    "# Plot the count of titles containing 'machine learning' over time\n",
    "fig = px.line(df_agg, x='published', y='title_has_word')\n",
    "fig.update_layout(\n",
    "    title_text=f\"{ml_sum} '{word}' Titles Over The Past Two Years\", \n",
    "    xaxis_title_text='Date', \n",
    "    yaxis_title_text=f\"{ml_sum} ArXiv Title Publication Count for '{word}'\", \n",
    "    bargap=0.2,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf868c19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:40:21.505980Z",
     "iopub.status.busy": "2023-07-27T03:40:21.505613Z",
     "iopub.status.idle": "2023-07-27T03:40:21.512122Z",
     "shell.execute_reply": "2023-07-27T03:40:21.510648Z",
     "shell.execute_reply.started": "2023-07-27T03:40:21.505950Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO huggingface_hub cedentials?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b2fccf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:40:21.514959Z",
     "iopub.status.busy": "2023-07-27T03:40:21.513849Z",
     "iopub.status.idle": "2023-07-27T03:40:40.002634Z",
     "shell.execute_reply": "2023-07-27T03:40:40.001142Z",
     "shell.execute_reply.started": "2023-07-27T03:40:21.514916Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model)\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "models = api.list_models()\n",
    "df_models = pd.DataFrame(models)\n",
    "# The 6th in the list of models \n",
    "print(df_models[0][5])\n",
    "model = SentenceTransformer('')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d13c57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:40:40.005437Z",
     "iopub.status.busy": "2023-07-27T03:40:40.004489Z",
     "iopub.status.idle": "2023-07-27T03:40:40.629677Z",
     "shell.execute_reply": "2023-07-27T03:40:40.627766Z",
     "shell.execute_reply.started": "2023-07-27T03:40:40.005388Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://huggingface.co/api/models'\n",
    "data = requests.get(url).text\n",
    "# xml = pd.read_xml()\n",
    "# print(xml)\n",
    "d = data.rindex('specter', 243203)\n",
    "print(d, type(d), data[d-245:d+24])\n",
    "# This appears to be the only specter model available now. SMH\n",
    "# AnonymousSub/specter-emanuals-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c684b5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:40:40.631878Z",
     "iopub.status.busy": "2023-07-27T03:40:40.631437Z",
     "iopub.status.idle": "2023-07-27T03:40:40.653239Z",
     "shell.execute_reply": "2023-07-27T03:40:40.651939Z",
     "shell.execute_reply.started": "2023-07-27T03:40:40.631843Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flow = d6tflow.Workflow()\n",
    "df = flow.outputLoad(SortMetaTask)\n",
    "def get_item(row):\n",
    "    return row[2]\n",
    "def expand_df(df):\n",
    "    # Needs to be sorted again \n",
    "    df['sort_key'] = df['articles'].apply(get_item)\n",
    "    df.sort_values('sort_key', inplace=True)\n",
    "    df.drop(columns='sort_key', inplace=True)\n",
    "    df_expanded = df['articles'].apply(pd.Series)\n",
    "    df_expanded.columns = ['url', 'update', 'pub date', 'title', 'abstract']\n",
    "    return df_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f1e327",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:40:40.655677Z",
     "iopub.status.busy": "2023-07-27T03:40:40.654835Z",
     "iopub.status.idle": "2023-07-27T03:40:40.665208Z",
     "shell.execute_reply": "2023-07-27T03:40:40.663605Z",
     "shell.execute_reply.started": "2023-07-27T03:40:40.655632Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def xml_data(xml):\n",
    "    root = ET.fromstring(xml)\n",
    "    archive = {'articles': []}\n",
    "    for children in root:\n",
    "        for child in children[2:3]:\n",
    "            archive['articles'].append([child.text for child in children[:5]])\n",
    "    data = pd.DataFrame(archive)\n",
    "    df = expand_df(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be6d2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:40:40.667460Z",
     "iopub.status.busy": "2023-07-27T03:40:40.667081Z",
     "iopub.status.idle": "2023-07-27T03:40:40.683425Z",
     "shell.execute_reply": "2023-07-27T03:40:40.682022Z",
     "shell.execute_reply.started": "2023-07-27T03:40:40.667429Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "working_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7a9cc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# The following embedding API - https://github.com/allenai/paper-embedding-public-apis - \n",
    "# will embed batches up to size 16 with SPECTER. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e4682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:40:40.686021Z",
     "iopub.status.busy": "2023-07-27T03:40:40.685108Z",
     "iopub.status.idle": "2023-07-27T03:40:41.077457Z",
     "shell.execute_reply": "2023-07-27T03:40:41.076475Z",
     "shell.execute_reply.started": "2023-07-27T03:40:40.685973Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "URL = \"https://model-apis.semanticscholar.org/specter/v1/invoke\"\n",
    "MAX_BATCH_SIZE = 16\n",
    "\n",
    "def chunks(lst, chunk_size=MAX_BATCH_SIZE):\n",
    "    \"\"\"Splits a longer list to respect batch size\"\"\"\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i : i + chunk_size]\n",
    "\n",
    "PAPERS = [\n",
    "    {\n",
    "        \"paper_id\": str(row_id),\n",
    "        \"title\": row['title'],\n",
    "        \"abstract\": row['abstract'],\n",
    "    } \n",
    "    for row_id, row in df.iterrows()\n",
    "]\n",
    "\n",
    "def embed(papers):\n",
    "    embeddings_by_paper_id: Dict[str, List[float]] = {}\n",
    "    for chunk in chunks(papers):\n",
    "        # Allow Python requests to convert the data above to JSON\n",
    "        response = requests.post(URL, json=chunk)\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(\"Sorry, something went wrong, please try later!\")\n",
    "        for paper in response.json()[\"preds\"]:\n",
    "            embeddings_by_paper_id[paper[\"paper_id\"]] = paper[\"embedding\"]\n",
    "    return embeddings_by_paper_id\n",
    "\n",
    "class Embeddings2Task(d6tflow.tasks.TaskPickle):\n",
    "    def run(self):\n",
    "        all_embeddings = embed(PAPERS)\n",
    "        self.save(all_embeddings)\n",
    "    \n",
    "flow = d6tflow.Workflow()\n",
    "flow.run(Embeddings2Task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5d5b7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "There's now a very nice new .pkl and an updated .json dataset in our working tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb490707",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Alright, we have wrangled some very nice embeddings that can be queried for similar scientific papers with a two part query that expects a title and an abstract to search for throughout the pretrained model corpora. We can get by for now with some short descriptive text in place of a title and abstract. \n",
    "\n",
    "We will update our dataset for this model with our original MetaDataTask output so that we can have search results for the gap years prior to two years ago and 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536be64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:40:41.080014Z",
     "iopub.status.busy": "2023-07-27T03:40:41.079184Z",
     "iopub.status.idle": "2023-07-27T03:40:41.110530Z",
     "shell.execute_reply": "2023-07-27T03:40:41.108972Z",
     "shell.execute_reply.started": "2023-07-27T03:40:41.079981Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SecondSortTask(d6tflow.tasks.TaskPickle):\n",
    "    def requires(self):\n",
    "        return MetaDataTask()\n",
    "    \n",
    "    def run(self): \n",
    "        df = self.inputLoad()\n",
    "        df = df['meta']['data']\n",
    "        root = ET.fromstring(df)\n",
    "        archive = {'articles': []}\n",
    "        date_list = []\n",
    "        abstract_list = []\n",
    "        word = query_words[:2]\n",
    "        word = ' '.join(word)\n",
    "        \n",
    "        for children in root:\n",
    "            for child in children[2:3]:\n",
    "                date_list.append(child.text)\n",
    "\n",
    "            for child in children[3:4]:\n",
    "                archive['articles'].append(child.text)\n",
    "\n",
    "            for child in children[4:5]:\n",
    "                abstract_list.append(child.text)\n",
    "                \n",
    "        # Add dates to the article dict, date is just pub date now \n",
    "        archive['date'] = date_list\n",
    "        df = pd.DataFrame()\n",
    "        df['date'] = archive['date']\n",
    "        \n",
    "        # Convert the dates to datetime objects\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df['title'] = archive['articles']\n",
    "        df['abstract'] = abstract_list\n",
    "        df['abstract'] = df['abstract'].str.lower()\n",
    "        df['abstract'] = df['abstract'].replace('\\n','', regex=True)\n",
    "\n",
    "        # Find words in the sorted articles \n",
    "        df['title'] = df['title'].str.lower()\n",
    "\n",
    "        # Create a new column that is True if 'text' contains the word, False otherwise\n",
    "        df['contains_search_string'] = df['title'].str.contains(word)\n",
    "        df = df.sort_values(by='date')\n",
    "        df['title'] = df['title'].replace('\\n','', regex=True)\n",
    "\n",
    "        # Histogram of article dates \n",
    "        fig = px.histogram(df, x='date')\n",
    "        fig.update_layout(\n",
    "            title_text=f'Distribution of Article Dates For Query: {query_words}', \n",
    "            xaxis_title_text='6 Month Period', \n",
    "            yaxis_title_text=f'Count For {query_size} Results', \n",
    "            bargap=0.3, # gap between bars \n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "        # Count the number of texts that do and do not contain the search string\n",
    "        search_counts = df['contains_search_string'].value_counts()\n",
    "        # Group by 'date' and sum the new column to get counts\n",
    "        counts = df.groupby('date')['contains_search_string'].sum().reset_index()\n",
    "        fig = px.histogram(counts, x='date', y ='contains_search_string', nbins=50)\n",
    "        fig.update_layout(\n",
    "            title_text=f'{search_counts[True]} \"{word}\" Article Titles Distribution by Date', \n",
    "            xaxis_title_text='Year', \n",
    "            yaxis_title_text=f'ArXiv Article Title Count For \"{word}\"', \n",
    "            bargap=0.2,\n",
    "        )\n",
    "        fig.show()\n",
    "        \n",
    "        # Combine the data\n",
    "        trace1 = go.Histogram(\n",
    "            x=df['date'],\n",
    "            opacity=1.0,\n",
    "            name='Your ArXiv Search Query Results By Year'\n",
    "        )\n",
    "        trace2 = go.Histogram(\n",
    "            x=counts.loc[counts['contains_search_string'] == True, 'date'],\n",
    "            nbinsx=50,\n",
    "            opacity=0.75,\n",
    "            name=f'{word} is in Result Title'\n",
    "        )\n",
    "        data = [trace1, trace2]\n",
    "        layout = go.Layout(barmode='group')\n",
    "        fig = go.Figure(data=data, layout=layout)\n",
    "        fig.show()\n",
    "\n",
    "        # Create an instance of CountVectorizer\n",
    "        vectorizer = CountVectorizer()\n",
    "        # Fit the vectorizer and transform the text\n",
    "        X = vectorizer.fit_transform(df['title'])\n",
    "        # Use TruncatedSVD to reduce the dimensionality to 2D\n",
    "        svd = TruncatedSVD(n_components=2)\n",
    "        X_2d = svd.fit_transform(X)\n",
    "        df['svd_x'] = X_2d[:, 0]\n",
    "        df['svd_y'] = X_2d[:, 1]\n",
    "\n",
    "\n",
    "        fig = go.Figure()\n",
    "        # Add points for texts not containing the search string\n",
    "        fig.add_trace(go.Scatter(x=df[~df['contains_search_string']]['svd_x'], \n",
    "                                 y=df[~df['contains_search_string']]['svd_y'], \n",
    "                                 mode='markers', \n",
    "                                 name=f'{search_counts[False]} Titles w/o \"{word}\"',\n",
    "                                 hovertext=df[~df['contains_search_string']]['title']))\n",
    "\n",
    "        # Add points for texts containing the search string\n",
    "        fig.add_trace(go.Scatter(x=df[df['contains_search_string']]['svd_x'], \n",
    "                                 y=df[df['contains_search_string']]['svd_y'], \n",
    "                                 mode='markers', \n",
    "                                 name=f'{search_counts[True]} Titles Containing \"{word}\"',\n",
    "                                 hovertext=df[df['contains_search_string']]['title']))\n",
    "        # Add axis labels\n",
    "        fig.update_layout(\n",
    "            title_text=f'Hover Title Text SVD Scatter Components', \n",
    "            xaxis_title=\"Component 1\",\n",
    "            yaxis_title=\"Component 2\",\n",
    "            font=dict(\n",
    "                size=18,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "        \n",
    "\n",
    "        # Get the feature names from the vectorizer\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        # Get the top words for each component\n",
    "        for i, component in enumerate(svd.components_):\n",
    "            top_words_idx = component.argsort()[-10:][::-1]  # Adjust number of words as needed\n",
    "            top_words = [feature_names[i] for i in top_words_idx]\n",
    "            print(f\"Component {i+1} top words: {top_words}\")\n",
    "            \n",
    "        self.save(df)\n",
    "        \n",
    "# flow = d6tflow.Workflow()\n",
    "# flow.run(SecondSortTask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc97f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:40:41.112643Z",
     "iopub.status.busy": "2023-07-27T03:40:41.112226Z",
     "iopub.status.idle": "2023-07-27T03:40:41.367879Z",
     "shell.execute_reply": "2023-07-27T03:40:41.366669Z",
     "shell.execute_reply.started": "2023-07-27T03:40:41.112583Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flow = d6tflow.Workflow()\n",
    "df = flow.outputLoad(SecondSortTask)\n",
    "all_embeddings = flow.outputLoad(Embeddings2Task)\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681caeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:40:41.370773Z",
     "iopub.status.busy": "2023-07-27T03:40:41.370337Z",
     "iopub.status.idle": "2023-07-27T03:40:43.789406Z",
     "shell.execute_reply": "2023-07-27T03:40:43.787821Z",
     "shell.execute_reply.started": "2023-07-27T03:40:41.370738Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize SPECTER embeddings \n",
    "embedded_df = pd.DataFrame(all_embeddings)\n",
    "embedded_df.to_json('specter_embeddings.json')\n",
    "# Transpose the dataframe, as PCA expects rows to represent samples (documents in this case)\n",
    "df_transposed = embedded_df.transpose()\n",
    "pca = PCA(n_components=2)\n",
    "# Apply PCA to the embeddings\n",
    "reduced_embeddings = pca.fit_transform(df_transposed)\n",
    "df_reduced = pd.DataFrame(reduced_embeddings, columns=['Dimension 1', 'Dimension 2'])\n",
    "df_reduced[f'Contains Search String {word}'] = df['contains_search_string'].values\n",
    "df_r_len = len(df_reduced)\n",
    "# Make 'date' column and two_years_ago date timezone naive\n",
    "df['date'] = df['date'].dt.tz_localize(None)\n",
    "two_years_ago = two_years_ago.replace(tzinfo=None)\n",
    "print(df_r_len)\n",
    "df_reduced['Published Within Last Two Years'] = df['date'].apply(lambda x: x > two_years_ago)\n",
    "df_reduced['Not Published Within Last Two Years'] = df['date'].apply(lambda x: x < two_years_ago)\n",
    "true_count = df_reduced['Published Within Last Two Years'].value_counts()\n",
    "print(true_count)\n",
    "df_len = len(df_reduced)\n",
    "df_new = df_reduced[df_reduced['Not Published Within Last Two Years'] == False]\n",
    "two_counts = df_reduced['Not Published Within Last Two Years'].value_counts()\n",
    "print(df_new, two_counts)\n",
    "date = df['date']\n",
    "fig = px.scatter(df_reduced, x=date, y='Dimension 2', color=f'Contains Search String {word}')\n",
    "fig.update_layout(\n",
    "        title_text=f'1D (Dimension 2) PCA Scatter For All {df_r_len} SPECTER Embeddings'\n",
    "      \n",
    ")    \n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(df_reduced, x='Dimension 1', y='Dimension 2', color=f'Contains Search String {word}')\n",
    "fig.update_layout(\n",
    "        title_text=f' PCA Scatter For All {df_r_len} SPECTER Embeddings'\n",
    ")    \n",
    "fig.show()   \n",
    "\n",
    "fig = px.scatter(df_reduced, x='Dimension 1', y='Dimension 2', color='Published Within Last Two Years')\n",
    "fig.update_layout(\n",
    "        title_text=f'2D PCA Scatter For {df_r_len} Embeddings'\n",
    ")    \n",
    "fig.show()\n",
    "df_new_len = len(df_new)\n",
    "fig = px.scatter(df_new, x='Dimension 1', y='Dimension 2', color=f'Contains Search String {word}')\n",
    "fig.update_layout(\n",
    "        title_text=f'2D PCA Scatter For Within Two Years, Total Count: {df_new_len}'\n",
    ")    \n",
    "fig.show()\n",
    "\n",
    "df_new = df_reduced[df_reduced['Not Published Within Last Two Years'] == True]\n",
    "df_new_len = len(df_new)\n",
    "fig = px.scatter(df_new, x='Dimension 1', y='Dimension 2', color=f'Contains Search String {word}')\n",
    "fig.update_layout(\n",
    "        title_text=f'2D PCA Scatter For {word} Embeddings Prior To Two Years Ago From, Total Count: {df_new_len}'\n",
    ")    \n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8615347",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:47:47.248409Z",
     "iopub.status.busy": "2023-07-27T03:47:47.247014Z",
     "iopub.status.idle": "2023-07-27T03:47:47.254748Z",
     "shell.execute_reply": "2023-07-27T03:47:47.253193Z",
     "shell.execute_reply.started": "2023-07-27T03:47:47.248353Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = 'search_results.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb1b76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:50:51.026214Z",
     "iopub.status.busy": "2023-07-27T03:50:51.025564Z",
     "iopub.status.idle": "2023-07-27T03:50:55.145672Z",
     "shell.execute_reply": "2023-07-27T03:50:55.144655Z",
     "shell.execute_reply.started": "2023-07-27T03:50:51.026157Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = 'specter_embeddings.json'\n",
    "df = pd.read_json(embeddings)\n",
    "df = pd.DataFrame(df)\n",
    "corpus_embeddings = df.transpose().values.tolist()\n",
    "corpus_embeddings = torch.tensor(corpus_embeddings)\n",
    "print(corpus_embeddings)\n",
    "\n",
    "\n",
    "def sentence_transformer(text):\n",
    "    model = SentenceTransformer('sentence-transformers/allenai-specter')\n",
    "    embedding = model.encode(text, convert_to_tensor=True)\n",
    "    return embedding \n",
    "\n",
    "\n",
    "class SPECTERTask(d6tflow.tasks.TaskPickle):   \n",
    "    def requires(self):\n",
    "        return {'task': ClusterTask(), \n",
    "                'task1': MetaDataTask(),\n",
    "               } \n",
    "    # Just a different way to load from a dict of previous task outputs    \n",
    "    def run(self):\n",
    "        data = self.input()['task'].load()\n",
    "        data_task1 = self.input()['task1'].load()\n",
    "\n",
    "        xml = data_task1['meta']['data']\n",
    "        df_xml = xml_data(xml)\n",
    "        df_xml.to_json('sorted_results.json')\n",
    "        # Two years of embeddings as a single embedding\n",
    "        df = data\n",
    "        df['combined_has_word'] = df['combined'].str.contains(f'{word}')\n",
    "        print(df)\n",
    "        two_years = df[df['two_year_date'] == 1]\n",
    "        text = two_years['combined'].tolist()\n",
    "        two_year_embedding = sentence_transformer(text)\n",
    "        # Prior to two years ago for each value of contains {word}  \n",
    "        df = df[df['two_year_date'] == 0]        \n",
    "        text = df[df['combined_has_word'] == False]['combined'].values\n",
    "        array0_embedding = sentence_transformer(text)\n",
    "        text = df[df['combined_has_word'] == True]['combined'].values\n",
    "        array1_embedding = sentence_transformer(text)\n",
    "        data = {'embedding': two_year_embedding, 'array0_embedding': array0_embedding, 'array1_embedding': array1_embedding}\n",
    "        self.save(data)\n",
    "        \n",
    "    # The following modified function can be found here: \n",
    "    # https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/semantic-search/semantic_search_publications.py\n",
    "\n",
    "    def semantic_search_papers(title, abstract):\n",
    "        # SPECTERTask.search_papers() is going to take two strings\n",
    "        # ['url']['title']['abstract']['venue']['year'] for .json  \n",
    "        papers = pd.read_json(results)\n",
    "        papers = pd.DataFrame(papers)\n",
    "        text = title+'[SEP]'+abstract\n",
    "        # df = pd.DataFrame(papers)\n",
    "        query_embedding = sentence_transformer(text)\n",
    "        search_hits = util.semantic_search(query_embedding, corpus_embeddings)\n",
    "        search_hits = search_hits[0] \n",
    "        print(search_hits)  \n",
    "        print(\"\\n\\nPaper:\", title)\n",
    "        print(\"Most similar papers:\")\n",
    "        for hit in search_hits:\n",
    "            i = hit['corpus_id']\n",
    "            related_paper = papers.loc[i]\n",
    "            print(\"{:.2f}\\t{}\\t{} {}\".format(hit['score'], related_paper['title'], related_paper['venue'], related_paper['year']))\n",
    "            \n",
    "    def specter2_search_papers(title, abstract):\n",
    "        # results = 'search_results.json'\n",
    "        # papers = pd.read_json(results)\n",
    "        # papers = pd.DataFrame(papers)\n",
    "        papers = pd.read_json(results)\n",
    "        papers = pd.DataFrame(papers)\n",
    "        papers = papers.to_dict()\n",
    "        papers_zip = zip(papers['title'], papers['abstract'])\n",
    "        print(papers_zip)\n",
    "        model = AutoAdapterModel.from_pretrained(\"alienai/specter2\")\n",
    "        adapter_name = model.load_adapter(\"alienai/specter2_proximity\", source=\"hf\", set_active=True)\n",
    "        # load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained('alienai/specter2')\n",
    "        text_batch = [f\"{title}{tokenizer.sep_token}{abstract}\" for title, abstract in papers_zip]\n",
    "\n",
    "        # preprocess the input\n",
    "        inputs = tokenizer(text_batch, padding=True, truncation=True,\n",
    "                                     return_tensors=\"pt\", return_token_type_ids=False, max_length=512)\n",
    "        output = adapter_name(**inputs)\n",
    "        # take the first token in the batch as the embedding\n",
    "        corpus_embeddings = output.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        \n",
    "\n",
    "flow = d6tflow.Workflow()\n",
    "flow.run(SPECTERTask)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3cec3c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d03c8d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "\\begin{equation} \\cos ({\\bf t},{\\bf e})= {{\\bf t} {\\bf e} \\over \\|{\\bf t}\\| \\|{\\bf e}\\|} = \\frac{ \\sum_{i=1}^{n}{{\\bf t}_i{\\bf e}_i} }{ \\sqrt{\\sum_{i=1}^{n}{({\\bf t}_i)^2}} \\sqrt{\\sum_{i=1}^{n}{({\\bf e}_i)^2}} } \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238a34a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Papers Triplet For Citation Based Model Training \n",
    "$\\begin{equation}$\n",
    "$\\mathcal{L}$ = \n",
    "$\\max \\left\\{ \\left( d(P_Q, P_+)  - d(P_Q, P_-) + m \\right), 0 \\right\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f5411f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "With the SPECTER transformers pretrained on document relatedness they can do inference with just the title and abstract text of the paper used as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f996a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:40:48.782382Z",
     "iopub.status.busy": "2023-07-27T03:40:48.781980Z",
     "iopub.status.idle": "2023-07-27T03:41:59.358476Z",
     "shell.execute_reply": "2023-07-27T03:41:59.357249Z",
     "shell.execute_reply.started": "2023-07-27T03:40:48.782346Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MasterTask(d6tflow.tasks.TaskPqPandas): \n",
    "    def requires(self):\n",
    "        return {'task1': MetaDataTask(), \n",
    "                # 'task2': SortMetaTask(), \n",
    "                # 'task3': ClusterTask(),\n",
    "                'task4': SecondSortTask(), \n",
    "                'task5': VizMetaTask(), \n",
    "               }\n",
    "\n",
    "    def run(self):\n",
    "        data_task1 = self.input()['task1'].load()\n",
    "        # data_task2 = self.input()['task2'].load()\n",
    "        # data_task3 = self.input()['task3'].load()\n",
    "        data_task4 = self.input()['task4'].load()\n",
    "        data_task5 = self.input()['task5'].load()\n",
    "        data = {}\n",
    "       \n",
    "\n",
    "flow = d6tflow.Workflow()\n",
    "flow.run(MasterTask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7c3f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:41:59.360853Z",
     "iopub.status.busy": "2023-07-27T03:41:59.360319Z",
     "iopub.status.idle": "2023-07-27T03:41:59.381417Z",
     "shell.execute_reply": "2023-07-27T03:41:59.380104Z",
     "shell.execute_reply.started": "2023-07-27T03:41:59.360816Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flow = d6tflow.Workflow()\n",
    "df = flow.outputLoad(SPECTERTask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f551730a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# For exploring SPECTER further, try the alienai/specter github page - [https://github.com/allenai/specter](http://) \n",
    "Below is a visualization of the entire embedding space created by combining for the prior two years worth of results all title and abstract string text into one string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb7fd33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:41:59.384628Z",
     "iopub.status.busy": "2023-07-27T03:41:59.384214Z",
     "iopub.status.idle": "2023-07-27T03:41:59.423922Z",
     "shell.execute_reply": "2023-07-27T03:41:59.422609Z",
     "shell.execute_reply.started": "2023-07-27T03:41:59.384569Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "specter_embeddings = flow.outputLoad(SPECTERTask)\n",
    "print(specter_embeddings)\n",
    "# calculate cosine similarity\n",
    "embedding = specter_embeddings['embedding']\n",
    "array0_embedding = specter_embeddings['array0_embedding']\n",
    "cosine_scores = util.pytorch_cos_sim(embedding, array0_embedding)\n",
    "\n",
    "print('Cosine Scores: ', cosine_scores)\n",
    "\n",
    "\n",
    "def embeddings_array(embeddings):\n",
    "    indices = [[i] for i in range(len(embeddings.values))]\n",
    "    embeddings_tensor = torch.tensor(embeddings.values)\n",
    "    embeddings_tensor_2d = embeddings_tensor.unsqueeze(0)\n",
    "    embeddings_array_2d = embeddings_tensor_2d.numpy()\n",
    "    indices = np.array(indices)\n",
    "    embeddings_matrix = np.array(embeddings_array_2d)\n",
    "    # print(embeddings_matrix)\n",
    "\n",
    "# embeddings_array(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b7aa72",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "If the similarity is greater for the prior two years whole embedding and the prior to two years embedding of article titles and abstracts with \"machine learning\" in the text, then we know that articles from the prior two years have more semantic similarity with \"machine learning\" in the title and abstract texts than to articles without it prior to two years ago which tells us something about the last two years represented in the query results articles. However, the similarity between two sets prior to two years ago is lower than each against the whole recent two year set of articles and the scores for them against the two year set are very close. \n",
    "\n",
    "What we want to be sure of is whether articles from the last two years that don't contain \"machine learning\" in the text are more similar to older articles that don't or more recent articles that do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80173ee3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:41:59.427384Z",
     "iopub.status.busy": "2023-07-27T03:41:59.425959Z",
     "iopub.status.idle": "2023-07-27T03:41:59.432888Z",
     "shell.execute_reply": "2023-07-27T03:41:59.431632Z",
     "shell.execute_reply.started": "2023-07-27T03:41:59.427336Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8a57c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:41:59.435209Z",
     "iopub.status.busy": "2023-07-27T03:41:59.434417Z",
     "iopub.status.idle": "2023-07-27T03:41:59.457190Z",
     "shell.execute_reply": "2023-07-27T03:41:59.455534Z",
     "shell.execute_reply.started": "2023-07-27T03:41:59.435167Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flow = d6tflow.Workflow()\n",
    "df = flow.outputLoad(SecondSortTask)\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b34c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:41:59.459969Z",
     "iopub.status.busy": "2023-07-27T03:41:59.459121Z",
     "iopub.status.idle": "2023-07-27T03:41:59.470569Z",
     "shell.execute_reply": "2023-07-27T03:41:59.469560Z",
     "shell.execute_reply.started": "2023-07-27T03:41:59.459933Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "two_years_ago.date()\n",
    "# The regular indices are not sorted \n",
    "print(df['date'][0])\n",
    "# The iloc indices are sorted \n",
    "print(df['date'].iloc[0])\n",
    "print(df['date'].iloc[-1])\n",
    "oldest = df['date'].iloc[0]\n",
    "newest = df['date'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d02be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:41:59.473209Z",
     "iopub.status.busy": "2023-07-27T03:41:59.472379Z",
     "iopub.status.idle": "2023-07-27T03:41:59.485692Z",
     "shell.execute_reply": "2023-07-27T03:41:59.484530Z",
     "shell.execute_reply.started": "2023-07-27T03:41:59.473136Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_string = f'\"{word}\" occured on {unique_dates} different days in {ml_sum} titles within two years-to-date for the {query_size} results \\\n",
    "which spanned '\n",
    "prompt_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b4424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:41:59.488359Z",
     "iopub.status.busy": "2023-07-27T03:41:59.487799Z",
     "iopub.status.idle": "2023-07-27T03:41:59.499985Z",
     "shell.execute_reply": "2023-07-27T03:41:59.498678Z",
     "shell.execute_reply.started": "2023-07-27T03:41:59.488313Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_string = prompt_string + str(oldest) + ' to ' + str(newest)\n",
    "prompt_string = prompt_string + f' and two years ago began {two_years_ago}.'\n",
    "print(prompt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b23ec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:41:59.511645Z",
     "iopub.status.busy": "2023-07-27T03:41:59.510034Z",
     "iopub.status.idle": "2023-07-27T03:41:59.518216Z",
     "shell.execute_reply": "2023-07-27T03:41:59.516727Z",
     "shell.execute_reply.started": "2023-07-27T03:41:59.511491Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytz import utc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f538e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "It's immediately apparent that the results from the previous two years are more scattered over the whole shape of all 3000 embeddings in PCA. Prior to that period, the embeddings appear sparsely grouped into containing or not containing 'machine learning' in the title or abstract text. The clusters are noticible and this description is something I would like to see an LLM tackle in some decent nomenclature. It is also the kind of thing that cannot be used in place of a human when a human is taking ethical documented credit of their own thoughts, evaluations, conclustions, etc. in scientific research. This notebook is a helpful code document, but it's not a replacement for yourself. Learning from it is a less severe infraction than letting it do all the work. Learning from it is doable, but not necissarily dependable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec11c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:41:59.520862Z",
     "iopub.status.busy": "2023-07-27T03:41:59.519750Z",
     "iopub.status.idle": "2023-07-27T03:41:59.547810Z",
     "shell.execute_reply": "2023-07-27T03:41:59.546428Z",
     "shell.execute_reply.started": "2023-07-27T03:41:59.520811Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flow = d6tflow.Workflow()\n",
    "beds = flow.outputLoad(SPECTERTask)\n",
    "embeds = beds['embedding']\n",
    "print(embeds[:8], len(embeds), 'dimensions for two years prior articles as one SPECTER embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82f024c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Rather than compute the embeddings x embeddings matrix for pairwise cosine similarity, we can save some memory by trying Sentence Transformers [paraphrase-mining](https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/paraphrase-mining) which utilizes chunking and prioritizing the best similar pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0e9c5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Let's find the articles with the best matches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1addab",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "SPECTER results are expected to be slightly better (whatever that means) than the paraphrase miner, but this should be interesting nonetheless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fd293",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:41:59.550181Z",
     "iopub.status.busy": "2023-07-27T03:41:59.549264Z",
     "iopub.status.idle": "2023-07-27T03:41:59.573944Z",
     "shell.execute_reply": "2023-07-27T03:41:59.572940Z",
     "shell.execute_reply.started": "2023-07-27T03:41:59.550137Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = (df['title'] + '. ' + df['abstract']).tolist()\n",
    "# paraphrases = util.paraphrase_mining(model, sentences, corpus_chunk_size=len(sentences), top_k=1)\n",
    "# paraphrase-albert-small-v2 has tougher scores and is the smallest \n",
    "class ParaphraseTask(d6tflow.tasks.TaskPickle):\n",
    "    def run(self):\n",
    "        model = SentenceTransformer('paraphrase-albert-small-v2')\n",
    "        df = pd.DataFrame(df_read)\n",
    "        paraphrases = util.paraphrase_mining(model, sentences)\n",
    "        model = paraphrases \n",
    "        self.save(model)\n",
    "        # [df['url'], + [', '] + df['pub date']]\n",
    "flow = d6tflow.Workflow()\n",
    "flow.run(ParaphraseTask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b26a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:41:59.576182Z",
     "iopub.status.busy": "2023-07-27T03:41:59.575400Z",
     "iopub.status.idle": "2023-07-27T03:41:59.622234Z",
     "shell.execute_reply": "2023-07-27T03:41:59.621136Z",
     "shell.execute_reply.started": "2023-07-27T03:41:59.576147Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_read = pd.read_json('sorted_results.json')\n",
    "print(df_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c98007",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:41:59.624137Z",
     "iopub.status.busy": "2023-07-27T03:41:59.623774Z",
     "iopub.status.idle": "2023-07-27T03:41:59.807016Z",
     "shell.execute_reply": "2023-07-27T03:41:59.805723Z",
     "shell.execute_reply.started": "2023-07-27T03:41:59.624107Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flow = d6tflow.Workflow()\n",
    "paraphrases = flow.outputLoad(ParaphraseTask)\n",
    "df = df_read\n",
    "for paraphrase in paraphrases[95:100]:\n",
    "    score, i, j = paraphrase\n",
    "    # 3 duplicates and a conf paper are 0-3\n",
    "    print(f\"{sentences[i]} \\t\\t {sentences[j]}\\n Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020407ab",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Hold on. There appear to be many paraphrased duplicate articles in our ArXiv query results. Does ArXiv know these dupes are there? I'm guessing, \"no\". Do they care? I have no idea. \n",
    "Let's compare the SPECTER results. \n",
    "So, we revisit our first d6tflow task output and gather the papers in the gap between 2018 and July of 2021 where we started our collection. We want to keep these papers separate so that we can fine tune with only the original meta data from the most recent journal articles as planned. Our d6tflow Workflow object makes this very easy.\n",
    "The most recent dataset of the scientific paper data running on alienai/specter2 is SciRepEval which boasts state-of-the-art multi embeddings per document.[2] To update our model we will use Hugging Face Transformers. \n",
    "Before working on the LangChain prompt, we should see more into SPECTER and paraphrase mining. Using SPECTER as a discriminator layer is possibly enough to weed out any copycat work or data example articles that are matched too high and/or are obviously not authentic. \n",
    "\n",
    "**We can ask, \"How do the results relate to previous two years?\". **\n",
    "\n",
    "The following abstract and title text are a very good fit for my query idea in this notebook. The phrase \"machine learning\" is repeated several times across the paraphrase. This is where we want to set up a more permanent file structure or database for the active searches. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2436a03e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:44:31.675484Z",
     "iopub.status.busy": "2023-07-27T03:44:31.675042Z",
     "iopub.status.idle": "2023-07-27T03:44:33.835262Z",
     "shell.execute_reply": "2023-07-27T03:44:33.833704Z",
     "shell.execute_reply.started": "2023-07-27T03:44:31.675452Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This appears to be a real title and abstract https://arxiv.org/pdf/1907.08908.pdf\n",
    "SPECTERTask.semantic_search_papers(title='Techniques for Automated Machine Learning',\n",
    "              abstract='Automated machine learning (AutoML) aims to find optimal machine learning \\\n",
    "                solutions automatically given a machine learning problem. It could release the \\\n",
    "                burden of data scientists from the multifarious manual tuning process and \\\n",
    "                enable the access of domain experts to the off-the-shelf machine learning \\\n",
    "                solutions without extensive experience. In this paper, we review the current \\\n",
    "                developments of AutoML in terms of three categories, automated feature \\\n",
    "                engineering (AutoFE), automated model and hyperparameter learning (AutoMHL), \\\n",
    "                and automated deep learning (AutoDL). State-of-the-art techniques adopted in \\\n",
    "                the three categories are presented, including Bayesian optimization, \\\n",
    "                reinforcement learning, evolutionary algorithm, and gradient-based approaches. \\\n",
    "                We summarize popular AutoML frameworks and conclude with current open \\\n",
    "                challenges of AutoML.') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db0f6f7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The SPECTER sentence transformer just maps a util semantic search. Let's open up SPECTER2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4d2d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T03:51:02.529526Z",
     "iopub.status.busy": "2023-07-27T03:51:02.529066Z",
     "iopub.status.idle": "2023-07-27T03:51:03.855347Z",
     "shell.execute_reply": "2023-07-27T03:51:03.851886Z",
     "shell.execute_reply.started": "2023-07-27T03:51:02.529494Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/cmp-lg/9405014.pdf\n",
    "title = 'Classifying Cue Phrases in Text and Speech Using Machine Learning.'\n",
    "abstract = 'Cue phrases may be used in a discourse sense to explicitly signal discourse \\\n",
    "            structure, but also in a sentential sense to convey semantic rather than \\\n",
    "            structural information. This paper explores the use of machine learning for \\\n",
    "            classifying cue phrases as discourse or sentential. Two machine learning \\\n",
    "            programs (Cgrendel and C4.5) are used to induce classification rules from sets \\\n",
    "            of pre-classified cue phrases and their features. Machine learning is shown to \\\n",
    "            be an effective technique for not only automating the generation of \\\n",
    "            classification rules, but also for improving upon previous results.'\n",
    "\n",
    "SPECTERTask.specter2_search_papers(title=f'{title}', abstract=f'{abstract}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8879533",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Wow. The results for the SPECTER example are really different than the paraphrase pair for the same text. SPECTER is trained on citations, not on strict similarity like paraprase-albert-small-v2. We can get more similar hits with paraphrase mining of the SPECTER hit list which is returned when the search papers function is called via the SPECTERTask d6tflow task class. \n",
    "We have whole embeddings for a large span of articles across article publication dates. So, any embedding for a single article can be compared to the entire embedding for the prior two years worth of articles in our query results. \n",
    "There is also the newer alienai/specter2 model which we can see some more specific embedding based results, but it's not going to make it into this tutorial essay.  \n",
    "\n",
    "Seeing the paraphrase miner results reduce the number of total usable query results makes me squeamish as a data scientist. I think most of the duplicates are just twice submitted versions of the same work.  \n",
    "\n",
    "Thankfully, we have SPECTER which is going to return results based on citations rather than a similarity score. The similarity scores are usually quite low which is a good indicator that they are not just plagiarized replicas, but actually cited articles. We definitely want to clean the data for statistical analysis and modeling. This is quite interesting to have found in such a big project centered around science. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764c9eb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.082055Z",
     "iopub.status.idle": "2023-07-27T03:42:03.082544Z",
     "shell.execute_reply": "2023-07-27T03:42:03.082315Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.082294Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recursive searches for paraphrase miner and SPECTER \n",
    "# search string title in sentences and reorder a df by occurances -\n",
    "# return paraphrase miner pairs, return unique SPECTER results for unique items \n",
    "# save data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb285cfa",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.085476Z",
     "iopub.status.idle": "2023-07-27T03:42:03.086300Z",
     "shell.execute_reply": "2023-07-27T03:42:03.086041Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.086015Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(paraphrases[:2])\n",
    "print(sentences[0])\n",
    "# sentences is a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f6143",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.088132Z",
     "iopub.status.idle": "2023-07-27T03:42:03.088696Z",
     "shell.execute_reply": "2023-07-27T03:42:03.088473Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.088450Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve and rerank \n",
    "# https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/in_document_search_crossencoder.py#L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a335f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.090700Z",
     "iopub.status.idle": "2023-07-27T03:42:03.091211Z",
     "shell.execute_reply": "2023-07-27T03:42:03.091011Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.090989Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct the prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b556e5f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.092704Z",
     "iopub.status.idle": "2023-07-27T03:42:03.093195Z",
     "shell.execute_reply": "2023-07-27T03:42:03.092990Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.092969Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install langchain openai InstructorEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d10a46",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.094757Z",
     "iopub.status.idle": "2023-07-27T03:42:03.095251Z",
     "shell.execute_reply": "2023-07-27T03:42:03.095054Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.095034Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip freeze>requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ed4aae",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Let's see which articles are the best to summarize the previous two years worth of results with \"machine learning\" in the title. We need to find the hits with the highest cosine similarity to the whole two year embedding. Those will be the most recent articles that are most similar to the collection of results from two years prior. Then we will do the same for the older results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951a3d41",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.096634Z",
     "iopub.status.idle": "2023-07-27T03:42:03.097126Z",
     "shell.execute_reply": "2023-07-27T03:42:03.096929Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.096883Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.docstore.document import Document\n",
    "import openai \n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from InstructorEmbedding import INSTRUCTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a8799",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.100100Z",
     "iopub.status.idle": "2023-07-27T03:42:03.102829Z",
     "shell.execute_reply": "2023-07-27T03:42:03.102564Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.102536Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = ''\n",
    "docs = ''\n",
    "docs = [Document(page_content=t) for t in texts[:]]\n",
    "text_splitter = CharacterTextSplitter()\n",
    "template_string = '''The occrance of {topic} in title texts for a corpus of data in query results; a recent two year span analysis.'''\n",
    "\n",
    "\n",
    "title_template = PromptTemplate(\n",
    "            template=template_string,\n",
    "            input_variables = ['topic'],\n",
    ")\n",
    "\n",
    "template_string = '''Write an 8 paragraph essay based on this title TITLE: {title}'''\n",
    "\n",
    "abstract_template = PromptTemplate(\n",
    "            template = template_string,\n",
    "            input_variables = ['topic'],\n",
    "            \n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "title_chain = LLMChain(llm=llm, prompt=title_template, verbose=True, output_key='title')\n",
    "summary_chain = LLMChain(llm=llm, prompt=abstract_template, verbose=True, output_key='summary')\n",
    "sequential_chain = SequentialChains(chains=[title_chain, abstract_chain], verbose=True, \n",
    "                                    input_variable=['topic'], output_variables=['title', 'summary'])\n",
    "\n",
    "\n",
    "docs = ''\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "chain.run(docs)\n",
    "\n",
    "\n",
    "\n",
    "if prompt:\n",
    "    response = sequential_chain.run(topic=prompt)\n",
    "    print(response, sequential_chain.output_variables)\n",
    "    \n",
    "prompt = f'the main differences in the meta data for {word} within the \\\n",
    "                        last two years as opposed to prior than two years'\n",
    "prompt = '''Summary of two years worth of machine learning meta data ArXiv query results.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add447cc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.104702Z",
     "iopub.status.idle": "2023-07-27T03:42:03.105199Z",
     "shell.execute_reply": "2023-07-27T03:42:03.104992Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.104971Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = INSTRUCTOR('hkunlp/instructor-xl')\n",
    "model = SentenceTransformer(\"hkunlp/instructor-xl\")\n",
    "\n",
    "sentence = \"3D ActionSLAM: wearable person tracking in multi-floor environments\"\n",
    "instruction = \"Represent the Science title:\"\n",
    "embeddings = model.encode([[instruction,sentence]])\n",
    "sims = cosine_similarity(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sentences_a = [['Represent the Science sentence: ','Parton energy loss in QCD matter'], \n",
    "               ['Represent the Financial statement: ','The Federal Reserve on Wednesday raised its benchmark interest rate.']]\n",
    "sentences_b = [['Represent the Science sentence: ','The Chiral Phase Transition in Dissipative Dynamics'],\n",
    "               ['Represent the Financial statement: ','The funds rose less than 0.5 per cent on Friday']]\n",
    "\n",
    "embeddings_a = model.encode(sentences_a)\n",
    "embeddings_b = model.encode(sentences_b)\n",
    "similarities = cosine_similarity(embeddings_a,embeddings_b)\n",
    "print(sims, similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60876b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.106807Z",
     "iopub.status.idle": "2023-07-27T03:42:03.107270Z",
     "shell.execute_reply": "2023-07-27T03:42:03.107080Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.107058Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flow = d6tflow.Workflow()\n",
    "df = flow.outputLoad(SecondSortTask)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "model = INSTRUCTOR('hkunlp/instructor-large')\n",
    "dates = [f\"Date: {date} of Title : {title}\" for date, title in zip(df['date'], df['title'])]\n",
    "titles = [{\"instruction\": f'Represent the Science title: ', \"text\": [f'{title} {date}' for title, date in (zip(df['title'], df['date']))]}]\n",
    "title = str()\n",
    "abstract = str()\n",
    "# We can load a different model of embeddigns without losing current model assignment \n",
    "# titles_embeddings = model.encode(titles)\n",
    "texts_with_instructions = []\n",
    "pairs = [\n",
    "    {\"instruction\": \"Represent the Science sentence for clustering:\", \"text\":  \n",
    "     [f'{title}' for title, abstract in (zip(df['title'], df['abstract']))]},\n",
    "    {\"instruction\": \"Represent the Science abstract for clustering:\", \"text\":  \n",
    "     [f'{abstract}' for title, abstract in (zip(df['title'], df['abstract']))]}\n",
    "]\n",
    "\n",
    "# or pair in pairs:\n",
    "    # TODO fix this loop \n",
    "    # texts_with_instructions.append([pair[\"instruction\"], pair[\"text\"]])\n",
    "# cluster_embeddings = model.encode(texts_with_instructions)\n",
    "\n",
    "# query  = [['Represent the Science question for retrieving supporting documents: ',f'{title}']]\n",
    "# corpus = [['Represent the Science document for retrieval: ', f'{abstract}']]\n",
    "          \n",
    "# query_embeddings = model.encode(query)\n",
    "# corpus_embeddings = model.encode(corpus)\n",
    "# similarities = cosine_similarity(query_embeddings,corpus_embeddings)\n",
    "# retrieved_doc_id = np.argmax(similarities)\n",
    "# print(retrieved_doc_id)\n",
    "# print(similarities)\n",
    "# prepare texts with instructions for titles and abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8031508f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.109440Z",
     "iopub.status.idle": "2023-07-27T03:42:03.110129Z",
     "shell.execute_reply": "2023-07-27T03:42:03.109914Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.109890Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "working_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aead4b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We just made an ad hoc query to an old dataset which returned a result based on relevance to our "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307de78",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.112288Z",
     "iopub.status.idle": "2023-07-27T03:42:03.112859Z",
     "shell.execute_reply": "2023-07-27T03:42:03.112606Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.112568Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from pprint import pprint\n",
    "from transformers import ElectraTokenizer, ElectraForPreTraining\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc29768",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.115024Z",
     "iopub.status.idle": "2023-07-27T03:42:03.115492Z",
     "shell.execute_reply": "2023-07-27T03:42:03.115293Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.115272Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip freeze>requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aec848",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.116764Z",
     "iopub.status.idle": "2023-07-27T03:42:03.117173Z",
     "shell.execute_reply": "2023-07-27T03:42:03.116996Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.116979Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by date and calculate the daily count of titles with 'machine learning'\n",
    "ml_daily = df.groupby('date')['contains_search_string'].sum().reset_index()\n",
    "df['string_counter'] = df['contains_search_string'].cumsum()\n",
    "\n",
    "# Create the plot\n",
    "fig = px.line(df, x='date', y='string_counter', title=f'Count of Titles With \"{word}\" Over Time in Query: {query_words}')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7c65c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "ElectraForPreTraining is a variant of the BERT model but with a different pre-training task. Instead of the masked language modeling task used in BERT, ELECTRA uses a novel pre-training task called replaced token detection. It has a discriminator that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc42ec",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.118611Z",
     "iopub.status.idle": "2023-07-27T03:42:03.119088Z",
     "shell.execute_reply": "2023-07-27T03:42:03.118878Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.118857Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimilarSearchTask():\n",
    "    def requires():\n",
    "        return EmbeddingsTask()\n",
    "    \n",
    "    def run():\n",
    "        \n",
    "        EmbeddingsTask = self.inputLoad()\n",
    "        dataset_file = 'prior_articles.json'\n",
    "        if not os.path.exists(dataset_file):\n",
    "            working_directory()\n",
    "        with open(dataset_file) as fIn:\n",
    "            papers = json.load(fIn)\n",
    "        \n",
    "        # We want to find a real recent article to find out similar articles about \n",
    "        tensor = '/kaggle/working/data/EmbeddingsTask/EmbeddingsTask__99914b932b-data.pkl'\n",
    "\n",
    "        file = pickle.load(tensor)\n",
    "        texts = []\n",
    "        # Create a mapping from index to text\n",
    "        index_to_text = {i: text for i, text in enumerate(texts)}\n",
    "            \n",
    "            \n",
    "        EmbeddingsTask.search_papers('Meta Study Analysis Of Machine Learning', \"This paper is cutting edge and \\\n",
    "                                     has beaten the competition.\", file, papers)\n",
    "        \n",
    "    def nearest_neighbors():\n",
    "        \n",
    "        embeddings = np.array(embeddings)\n",
    "        # Create a NearestNeighbors model\n",
    "        model = NearestNeighbors(n_neighbors=5, algorithm='ball_tree')\n",
    "        model.fit(embeddings)\n",
    "        # Assuming 'new_embedding' is the embedding you want to search for\n",
    "        new_embedding = np.array([...])\n",
    "        # Find the nearest neighbors to your new embedding\n",
    "        distances, indices = model.kneighbors([new_embedding])\n",
    "        # 'indices' will be a 2D array containing the indices of the nearest neighbors\n",
    "        nearest_neighbors = indices[0]\n",
    "        # Use these indices to get the corresponding texts\n",
    "        similar_texts = [index_to_text[i] for i in nearest_neighbors]\n",
    "        print(similar_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae11696",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.120551Z",
     "iopub.status.idle": "2023-07-27T03:42:03.121030Z",
     "shell.execute_reply": "2023-07-27T03:42:03.120836Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.120815Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "working_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096a2189",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The task of autowriting an essay is tricky. How will we be able to assure accuracy of anything? Since alienai/specter2 has more data in its dataset it provides us the freedom to compare our essay results based on different models and data. \n",
    "\n",
    "We can also formulate a difference in our datasets for comparison to understand how much more or less each set has than the other while comparing the final results. Remember, we want the essay to improve upon article information prior to the last two years for the last two years worth of articles. This shouldn't be a prolem for a large language model prompt, e.g. ChatGPT4. However, we don't want to be ignorant of mistakes to correct or improvements to make. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19633c82",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We can now utilize the OpenAI API which has very nice text embeddings for categorizing semantic sentiment similarity, and of course, Chat-GPT4 for a text completion prompt model to write an essay. There are additional cutting edge libraries that we will need in order to deal with the text in our data source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b080ec15",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.123867Z",
     "iopub.status.idle": "2023-07-27T03:42:03.124414Z",
     "shell.execute_reply": "2023-07-27T03:42:03.124206Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.124184Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO gpt-3.5-turbo-16k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c2ca7",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.126264Z",
     "iopub.status.idle": "2023-07-27T03:42:03.126745Z",
     "shell.execute_reply": "2023-07-27T03:42:03.126513Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.126494Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO generate a citations list in APA for the analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97123720",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-27T03:42:03.130154Z",
     "iopub.status.idle": "2023-07-27T03:42:03.130792Z",
     "shell.execute_reply": "2023-07-27T03:42:03.130532Z",
     "shell.execute_reply.started": "2023-07-27T03:42:03.130507Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_reset(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21292a50",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "1. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel S. Weld (2020). SPECTER: Document-level Representation Learning using Citation-informed Transformers.\n",
    "[v4]. ArXiv. https://doi.org/10.48550/arXiv.2004.07180\n",
    "\n",
    "2. https://huggingface.co/transformers/v3.0.2/model_doc/auto.html\n",
    "\n",
    "3. Singh, A., D'Arcy, M., Cohan, A., Downey, D., & Feldman, S. (2022). SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. ArXiv, abs/2211.13308.\n",
    "\n",
    "4. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\n",
    "\n",
    "5. OpenAI. (Year). ChatGPT (Month Day version) [Large language model]. https://chat.openai.com\n",
    "\n",
    "6. https://d6tflow.readthedocs.io/en/latest/tasks.html#save-output-data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 185.288155,
   "end_time": "2023-07-27T03:54:54.863113",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-27T03:51:49.574958",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}